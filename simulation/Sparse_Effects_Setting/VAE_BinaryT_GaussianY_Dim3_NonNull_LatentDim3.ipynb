{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## strong confoundedness: large B and gamma\n",
    "## small true effect: small tau\n",
    "\n",
    "k = 500\n",
    "s = 3\n",
    "np.random.seed(123)\n",
    "B = np.random.uniform(low=-2, high=2, size=(k,s))\n",
    "# B  = np.zeros(shape = (k,s))\n",
    "# B = np.array([2, 0.5, -0.4, 0.2]).reshape(k, s)\n",
    "sigma2_tstar = 100\n",
    "\n",
    "gamma = np.array([100]*s).reshape(s,1)\n",
    "tau = np.random.uniform(low=-0.1, high=0.1, size=(k,1))\n",
    "sigma2_y = 10\n",
    "\n",
    "# set some tau to large values #\n",
    "nontrivial_effect_index = np.random.randint(low = 0, high = k, size = int(k*0.1))\n",
    "nontrivial_effect_index = np.unique(nontrivial_effect_index)\n",
    "nontrivial_effect = np.random.uniform(low=-2, high=2, size=(nontrivial_effect_index.shape[0],1))\n",
    "tau[nontrivial_effect_index] = nontrivial_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "u = np.random.multivariate_normal(mean = np.full(s, 0), cov=np.identity(s), size=n)\n",
    "tr_star = np.dot(u, np.transpose(B)) + \\\n",
    "          np.random.multivariate_normal(mean = np.full(k, 0), cov=sigma2_tstar*np.identity(k), size = n)\n",
    "tr = np.where(tr_star > 0, 1, 0)\n",
    "y = np.dot(tr, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n).reshape(n,1)\n",
    "# y = np.dot(tr_star, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([754.59642144+0.j, 694.21036223+0.j, 606.69565787+0.j,\n",
       "        -0.        +0.j,  -0.        -0.j,   0.        +0.j,\n",
       "         0.        +0.j,   0.        -0.j,  -0.        +0.j,\n",
       "        -0.        -0.j])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the first eigenvalue of BB is large, thus var(u|t) is smaller given sigma2_star=100\n",
    "BB = np.dot(B, np.transpose(B))\n",
    "BB_w, BB_v = np.linalg.eig(BB) ## eigenvalues, eigevector\n",
    "BB_w[0:10] ## the first eigenvalue, 654.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cov(u|t*):\n",
      "[[ 0.12583743  0.01137063 -0.00124313]\n",
      " [ 0.01137063  0.13200804 -0.00338941]\n",
      " [-0.00124313 -0.00338941  0.12658369]]\n"
     ]
    }
   ],
   "source": [
    "## some theoretical values #\n",
    "coef_mu_u_tstar = np.dot(np.transpose(B), np.linalg.inv(np.dot(B, np.transpose(B)) + sigma2_tstar*np.identity(k)))\n",
    "cov_u_tstar = np.identity(s) - np.dot(coef_mu_u_tstar, B)\n",
    "print('Cov(u|t*):')\n",
    "print(cov_u_tstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. Var(y|t) [Var(y|t*)]: 3989.0533932258177\n",
      "confounding fraction in residual of Y: 0.9974931395962305\n"
     ]
    }
   ],
   "source": [
    "var_y_t = np.dot(np.dot(np.transpose(gamma), cov_u_tstar), gamma) + sigma2_y\n",
    "print('Approx. Var(y|t) [Var(y|t*)]: ' + str(var_y_t[0,0]))\n",
    "print('confounding fraction in residual of Y: ' + str(1 - sigma2_y/var_y_t[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0079136 , -0.08092806,  0.08204695,  0.03360369,  0.00199419,\n",
       "       -0.01374685, -0.0897069 , -0.05714116,  0.07121584,  0.02444088,\n",
       "       -0.06122498, -0.06215936, -0.0299749 ,  0.02305931,  0.00848305,\n",
       "        0.00540698,  0.07378616,  0.07225029,  0.0690397 ,  0.04049034,\n",
       "       -0.02150996, -0.04613115,  0.09113853, -0.09094301,  0.02367661,\n",
       "        0.06922482, -0.04561742,  0.09574921,  0.07718479, -0.06013438,\n",
       "       -0.04593814,  0.07946058, -0.07814964, -0.06230457,  0.22737075,\n",
       "       -0.07059713,  0.01450903,  0.01510904,  0.01927088, -0.04985972,\n",
       "        0.06421617,  0.09408872, -0.0444228 ,  0.07309866, -0.04138959,\n",
       "       -0.03021458,  0.05708869, -1.72425897, -0.08763849, -0.02267348,\n",
       "       -0.07406336, -0.08408945,  0.0177456 , -0.06320505, -1.33897426,\n",
       "        0.04581771, -0.08140086,  0.09879365, -0.03824867,  0.00912585,\n",
       "        0.0583699 ,  0.92088713, -0.03176725, -0.05874625, -0.02428434,\n",
       "        0.04698733,  0.04288538, -0.06359616, -0.78220437,  0.03764135,\n",
       "        0.04762998,  0.02720739, -0.06103546,  0.07361581, -0.04231045,\n",
       "       -0.02601564,  0.01169956,  0.0718757 ,  0.02993006,  0.04033754,\n",
       "       -0.05672634, -0.02948764, -0.07262132,  0.04738068,  0.08975156,\n",
       "        0.04697365, -1.26953849,  0.03867307, -0.01335496, -1.80573921,\n",
       "       -1.71966913,  0.04482506,  0.02448759,  0.07461957, -0.03493178,\n",
       "        0.03336902, -0.00634391, -0.07521005,  0.00821394,  0.0263819 ,\n",
       "       -0.59146753, -0.07494569, -0.08494337,  0.03296769, -1.94806865,\n",
       "        0.08481881, -0.02236283,  0.08766317,  0.06396069, -0.01519167,\n",
       "       -0.01873599, -0.03330424, -0.08617668, -0.09805477, -0.05090027,\n",
       "       -0.04907653,  0.04593221, -0.06852586,  0.03848433,  0.04606706,\n",
       "       -0.00805657, -0.0764693 ,  0.04390192,  0.08272945, -0.02676411,\n",
       "        0.05174796,  0.04838374, -0.08919157, -0.08885847,  0.77538658,\n",
       "        1.95423119,  0.08411238, -0.07166738,  0.01667804, -0.04358938,\n",
       "       -0.05545983,  0.04767771, -0.01736162, -0.01246158,  0.06888025,\n",
       "       -0.05226305,  0.00566848,  0.06554208, -0.07362224,  0.01923659,\n",
       "        0.06743802,  0.05289311,  0.07689642, -0.09348716, -1.65303916,\n",
       "       -0.0592127 ,  0.01140375,  0.05495578, -0.01596171,  0.04084074,\n",
       "        0.06808214,  0.08914374,  0.00901862, -0.03678001, -0.02199477,\n",
       "        0.70915755, -0.05193667,  0.07665288, -0.08843275,  0.0942651 ,\n",
       "       -0.07617856,  0.07667781,  0.00815434,  0.01226555,  0.09613119,\n",
       "        0.09027242,  0.05972163,  0.03031669,  0.03554028, -0.07330629,\n",
       "       -0.00651893, -0.01416684, -0.07961818, -0.09773431, -0.09674634,\n",
       "        0.04749332, -0.0105088 , -0.00188508, -0.04928868,  0.00816767,\n",
       "       -0.07455859, -0.06729005,  0.06352881, -0.09451413,  0.0832027 ,\n",
       "       -0.03236242,  0.08643168, -0.08726024, -0.77705981,  0.00406989,\n",
       "        0.08756097, -0.08199929,  0.06183861,  0.02260478,  0.02800161,\n",
       "        0.09846182, -0.03578344, -0.02796241,  0.00717078,  0.02501677,\n",
       "       -0.03765078,  0.04126471, -0.08357508, -0.06103904, -0.09734963,\n",
       "        0.04992402,  0.08957962,  0.0537422 , -0.06542578, -0.04322843,\n",
       "        0.03423566, -0.03811068, -0.01863631, -0.04883921, -0.16177302,\n",
       "        0.07389526, -0.05206143,  0.00590745,  0.03300073,  1.64104514,\n",
       "       -0.04940545,  0.01345348,  0.0650399 , -0.0544077 ,  0.07976955,\n",
       "        0.14563839, -1.76995231,  0.06269499,  0.06984971, -0.04366796,\n",
       "       -0.07696973, -1.46567845,  0.07615003, -0.00111956, -1.0285786 ,\n",
       "       -0.98942204,  0.05605043,  0.06484264,  0.07850714,  0.02835396,\n",
       "        0.07842408,  0.07337062, -0.04567125,  0.01770958, -0.04756825,\n",
       "        0.02756383,  0.0794295 , -0.00337208,  0.06491203, -0.09657909,\n",
       "       -1.2312683 ,  0.08884063,  0.83302244,  0.0419291 ,  0.06384619,\n",
       "       -0.07514641, -0.01711081, -0.05868332,  0.04931178, -0.01881703,\n",
       "       -0.09997281, -0.04747327, -0.05933951, -0.08504635,  0.67791959,\n",
       "        0.45247182, -0.05621372, -0.0711157 , -0.03524437, -0.07445948,\n",
       "        0.00463566,  0.07846723,  0.05559157, -0.07036216, -0.04020389,\n",
       "       -0.05256627,  0.0855272 , -0.04414265, -0.06612364, -0.00897657,\n",
       "       -0.98926708, -0.06829755, -0.09509788, -0.081047  ,  0.03018955,\n",
       "       -0.06731033, -0.04823132,  0.03514375,  0.07427242,  0.09053434,\n",
       "       -0.02328344, -0.05472571,  0.04782645,  0.04248478, -1.2222459 ,\n",
       "       -0.08042239,  0.07562887, -0.06486795,  0.04946708,  0.09908506,\n",
       "       -0.05364431,  0.04843596,  0.0548315 , -0.04380813,  0.08424609,\n",
       "       -0.23054374, -0.05422401, -0.04108412, -0.08198931, -0.05094956,\n",
       "        0.05743268,  1.91208518,  0.00132314,  1.48641395,  0.01833742,\n",
       "        0.02217407, -0.05634783, -0.08506975,  0.04035694,  0.02668726,\n",
       "       -0.00354997,  0.07908452,  0.09545388,  0.04647883,  0.09245467,\n",
       "       -0.07057108, -0.06809086,  0.50995641, -0.02469233,  0.01189202,\n",
       "        0.08894644,  0.02079495,  0.07400833,  0.04145928,  0.07117538,\n",
       "       -0.00594058,  0.08987605, -0.07494777,  0.06153423, -0.05294155,\n",
       "        0.02376282,  0.05525244,  0.78777153, -1.47880466,  1.89085774,\n",
       "       -0.06763414, -1.5461094 , -0.04936391,  0.03759285, -0.06187572,\n",
       "       -0.0904438 ,  0.05695847,  0.06348925, -0.06751403,  0.06141991,\n",
       "       -0.03439159,  0.01422948,  0.0203764 ,  0.08272648,  0.0140461 ,\n",
       "        0.0737913 , -0.01569594,  0.03063446, -1.36082929,  0.04244286,\n",
       "        0.08555092, -0.01456473,  0.02945791,  0.00885959, -0.09709982,\n",
       "        0.08155659,  0.06185807, -0.01690305, -0.08180634, -0.02968093,\n",
       "        0.00512836, -0.05116533,  0.06650102, -0.04090459, -0.05901336,\n",
       "       -0.0654272 ,  0.04525392,  0.00932454,  0.03919488, -0.04193098,\n",
       "       -1.16390905,  0.07912145, -0.03391951,  0.08058983,  0.09325709,\n",
       "       -0.09609319, -0.00115391,  0.0244112 ,  1.43476655, -0.02996715,\n",
       "       -0.09439128,  0.0232983 , -0.00736834, -0.047417  ,  0.05551806,\n",
       "        0.00858288, -0.03351455,  0.08706058,  0.09516687, -0.06351979,\n",
       "        0.09567836,  0.0175208 ,  0.07507169, -0.04170049, -0.09067845,\n",
       "       -0.09748298, -0.01381378, -1.04635961, -0.01090371,  0.04879405,\n",
       "        0.03366685,  0.0463911 ,  0.01891618,  0.09079796, -0.05197947,\n",
       "        0.0807653 ,  0.09634574, -0.09929039, -0.01377244,  0.03521444,\n",
       "        0.02290711,  1.7091457 ,  0.03207362,  1.38186164,  0.08526586,\n",
       "        0.08042289,  0.06683719, -0.01852506,  0.03949964,  0.09119998,\n",
       "       -0.05202279, -0.00240375,  0.05151424,  0.87245279,  0.05674962,\n",
       "       -0.04719791, -0.07383491, -0.00575748,  0.03296292,  0.01950928,\n",
       "        0.0221351 , -0.08146763, -0.00173624, -0.05950699,  0.04213561,\n",
       "        0.01168383,  0.02015551,  0.01911338, -0.0387573 , -0.08332616,\n",
       "       -0.01738219, -0.04045172,  0.0139415 , -0.05537245, -0.05525964,\n",
       "       -0.0349688 , -0.0653351 , -0.01959785,  0.06480344,  0.07040088,\n",
       "        1.11250327, -0.03385397,  0.02264242, -0.0546103 ,  0.08389538,\n",
       "        0.04132667, -0.05594201,  0.00163242, -0.08524483, -0.06240518,\n",
       "       -0.02089042,  0.05015773, -0.07180021, -0.09270756,  0.07834011,\n",
       "       -0.03292489, -0.03392768,  0.00502148, -0.01603405, -0.04095397,\n",
       "        0.04311586,  0.05843372,  0.06525593,  0.05400606,  0.03808556,\n",
       "       -1.41772389,  0.04633008, -0.04505772,  0.0443635 , -0.08042742])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True Treatment Effect\n",
    "tau.reshape(k,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect bias range:\n",
      "-0.7511657117030033\n",
      "0.7201117743876483\n",
      "obs effect range:\n",
      "-2.1567691647786456\n",
      "1.860814044224537\n"
     ]
    }
   ],
   "source": [
    "# Approx. Obs. Effect #\n",
    "effect_bias = np.dot(np.dot(np.transpose(gamma), coef_mu_u_tstar), \\\n",
    "                     np.identity(k)).reshape(k,)\n",
    "print('effect bias range:')\n",
    "print(effect_bias.min())\n",
    "print(effect_bias.max())\n",
    "effect_obs = tau.reshape(k,) + effect_bias\n",
    "print('obs effect range:')\n",
    "print(effect_obs.min())\n",
    "print(effect_obs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Bias for Nontrivial Ones:     [-0.19156168 -0.00601164  0.50798291 -0.16353192 -0.13218608  0.07048387\n",
      " -0.25363315 -0.17817511  0.03897286 -0.44606751 -0.08022859  0.1626099\n",
      " -0.21195302  0.31633593 -0.17574154 -0.387714   -0.43710003 -0.22362933\n",
      "  0.02218435 -0.43710003  0.02260092  0.0250833  -0.20360345  0.29173772\n",
      " -0.35123563 -0.29887068  0.17314676  0.29568091 -0.11594583 -0.13336539\n",
      "  0.01345059  0.29568091 -0.59743655 -0.41687289 -0.30065269 -0.09341714\n",
      " -0.09134576 -0.10699182 -0.32643323 -0.50785215  0.37873456 -0.02648453\n",
      " -0.3792888   0.30177889  0.16160004 -0.16403011 -0.23671804  0.1332102\n",
      " -0.15640847  0.38417182]\n",
      "Observed Effect for Nontrivial Ones: [ 0.03580907 -1.73027062 -0.83099135  0.75735521 -0.91439045 -1.19905462\n",
      " -2.05937236 -1.89784424 -0.55249467 -2.39413616  0.69515799  2.11684109\n",
      " -1.86499218  1.02549347 -0.95280136 -0.38364411 -2.20705234  1.4174158\n",
      "  0.16782274 -2.20705234 -1.44307753 -1.0034953  -1.19302549 -0.93953058\n",
      "  0.48178681  0.37904891  0.62561858  2.20776609 -1.10521291 -1.35561128\n",
      " -0.21709315  2.20776609  0.8889774   0.09308352  0.48711883 -1.5722218\n",
      "  1.79951197 -1.65310123 -1.68726251 -1.6717612   1.81350111 -1.07284413\n",
      "  1.3298569   1.68364053  1.03405283  0.94847316 -0.23508562 -1.28451368\n",
      " -0.07296996  1.30025698]\n",
      "True Effect for Nontrivial Ones:     [ 0.22737075 -1.72425897 -1.33897426  0.92088713 -0.78220437 -1.26953849\n",
      " -1.80573921 -1.71966913 -0.59146753 -1.94806865  0.77538658  1.95423119\n",
      " -1.65303916  0.70915755 -0.77705981  0.00406989 -1.76995231  1.64104514\n",
      "  0.14563839 -1.76995231 -1.46567845 -1.0285786  -0.98942204 -1.2312683\n",
      "  0.83302244  0.67791959  0.45247182  1.91208518 -0.98926708 -1.2222459\n",
      " -0.23054374  1.91208518  1.48641395  0.50995641  0.78777153 -1.47880466\n",
      "  1.89085774 -1.5461094  -1.36082929 -1.16390905  1.43476655 -1.04635961\n",
      "  1.7091457   1.38186164  0.87245279  1.11250327  0.00163242 -1.41772389\n",
      "  0.08343851  0.91608516]\n"
     ]
    }
   ],
   "source": [
    "print('Effect Bias for Nontrivial Ones:     ' + str(effect_bias[nontrivial_effect_index]))\n",
    "print('Observed Effect for Nontrivial Ones: ' + str(effect_obs[nontrivial_effect_index]))\n",
    "print('True Effect for Nontrivial Ones:     ' + str(tau[nontrivial_effect_index].reshape(50,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(tau).to_csv('tau.csv', index = False)\n",
    "# pd.DataFrame(u).to_csv('u.csv', index = False)\n",
    "# pd.DataFrame(tr).to_csv('tr.csv', index = False)\n",
    "# pd.DataFrame(y).to_csv('y.csv', index = False)\n",
    "# pd.DataFrame(nontrivial_effect_index).to_csv('nontrivial_effect_index.csv', index = False)\n",
    "n = 50000\n",
    "u = pd.read_csv('u.csv').to_numpy()\n",
    "tr = pd.read_csv('tr.csv').to_numpy()\n",
    "y = pd.read_csv('y.csv').to_numpy()\n",
    "tau = pd.read_csv('tau.csv').to_numpy()\n",
    "nontrivial_effect_index = pd.read_csv('nontrivial_effect_index.csv').to_numpy().reshape(50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting/LatentDim3')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "original_dim = k\n",
    "latent_dim = s\n",
    "intermediate_dim = 10\n",
    "epochs = 300\n",
    "epsilon_std = 1\n",
    "z_log_sigma_prior = np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder #\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim)(x)\n",
    "h = LeakyReLU(alpha = 0.1)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "# z_log_sigma = Dense(latent_dim)(h)\n",
    "## log_z_scale\n",
    "\n",
    "\n",
    "z_log_sigma_input = Input(batch_shape = (batch_size, 1))\n",
    "z_log_sigma = Dense(units = 1,  activation = \"linear\",\n",
    "                    kernel_initializer=initializers.Ones(),\n",
    "                    use_bias = False)(z_log_sigma_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from latent space #\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "# z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder #\n",
    "decoder_h1 = Dense(intermediate_dim)\n",
    "decoder_h2 = LeakyReLU(alpha = 0.1)\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h1(z)\n",
    "h_decoded = decoder_h2(h_decoded)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model([x, z_log_sigma_input], x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder_z_mean = Model([x, z_log_sigma_input], z_mean)\n",
    "encoder_z_log_sigma = Model([x, z_log_sigma_input], z_log_sigma)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h1(decoder_input)\n",
    "_h_decoded = decoder_h2(_h_decoded)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_metric(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    return xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.RMSprop(learning_rate=0.0005)\n",
    "# opt = optimizers.Adam(learning_rate=0.001)\n",
    "vae.compile(optimizer=opt, loss=vae_loss, metrics = [recon_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tr_train, tr_test = train_test_split(tr,train_size=0.8)\n",
    "log_sigma_input_train = np.full((tr_train.shape[0], 1), z_log_sigma_prior)\n",
    "log_sigma_input_test = np.full((tr_test.shape[0], 1), z_log_sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "40000/40000 [==============================] - 2s 49us/step - loss: 346.8502 - recon_metric: 346.6636 - val_loss: 346.7291 - val_recon_metric: 346.6055\n",
      "Epoch 2/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 346.6751 - recon_metric: 346.5442 - val_loss: 346.5910 - val_recon_metric: 346.4373\n",
      "Epoch 3/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 346.4131 - recon_metric: 346.1924 - val_loss: 346.1993 - val_recon_metric: 345.9050\n",
      "Epoch 4/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 345.8714 - recon_metric: 345.4674 - val_loss: 345.5515 - val_recon_metric: 345.0258\n",
      "Epoch 5/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 345.1980 - recon_metric: 344.6090 - val_loss: 344.8254 - val_recon_metric: 344.1248\n",
      "Epoch 6/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 344.4634 - recon_metric: 343.7009 - val_loss: 344.0968 - val_recon_metric: 343.2914\n",
      "Epoch 7/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 343.8077 - recon_metric: 342.9164 - val_loss: 343.4950 - val_recon_metric: 342.5497\n",
      "Epoch 8/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 343.2740 - recon_metric: 342.3012 - val_loss: 343.0107 - val_recon_metric: 342.0373\n",
      "Epoch 9/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 342.8328 - recon_metric: 341.8095 - val_loss: 342.6225 - val_recon_metric: 341.6152\n",
      "Epoch 10/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 342.4786 - recon_metric: 341.4210 - val_loss: 342.2644 - val_recon_metric: 341.2173\n",
      "Epoch 11/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 342.1807 - recon_metric: 341.1037 - val_loss: 342.0295 - val_recon_metric: 340.9741\n",
      "Epoch 12/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.9636 - recon_metric: 340.8782 - val_loss: 341.8605 - val_recon_metric: 340.7400\n",
      "Epoch 13/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 341.7813 - recon_metric: 340.6967 - val_loss: 341.7358 - val_recon_metric: 340.6204\n",
      "Epoch 14/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 341.6332 - recon_metric: 340.5593 - val_loss: 341.5295 - val_recon_metric: 340.4532\n",
      "Epoch 15/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 341.5006 - recon_metric: 340.4410 - val_loss: 341.4504 - val_recon_metric: 340.3800\n",
      "Epoch 16/300\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 341.3925 - recon_metric: 340.3486 - val_loss: 341.3449 - val_recon_metric: 340.2778\n",
      "Epoch 17/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.2975 - recon_metric: 340.2683 - val_loss: 341.2124 - val_recon_metric: 340.1844\n",
      "Epoch 18/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 341.2030 - recon_metric: 340.1900 - val_loss: 341.1526 - val_recon_metric: 340.1222\n",
      "Epoch 19/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.1317 - recon_metric: 340.1312 - val_loss: 341.0329 - val_recon_metric: 340.0303\n",
      "Epoch 20/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 341.0536 - recon_metric: 340.0666 - val_loss: 340.9855 - val_recon_metric: 339.9901\n",
      "Epoch 21/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.9951 - recon_metric: 340.0194 - val_loss: 340.9512 - val_recon_metric: 339.9488\n",
      "Epoch 22/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.9288 - recon_metric: 339.9617 - val_loss: 340.8872 - val_recon_metric: 339.9062\n",
      "Epoch 23/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.8861 - recon_metric: 339.9278 - val_loss: 340.8342 - val_recon_metric: 339.8749\n",
      "Epoch 24/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.8278 - recon_metric: 339.8782 - val_loss: 340.7716 - val_recon_metric: 339.8039\n",
      "Epoch 25/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.7791 - recon_metric: 339.8371 - val_loss: 340.7641 - val_recon_metric: 339.8080\n",
      "Epoch 26/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.7360 - recon_metric: 339.8000 - val_loss: 340.6668 - val_recon_metric: 339.7191\n",
      "Epoch 27/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.6925 - recon_metric: 339.7633 - val_loss: 340.6731 - val_recon_metric: 339.7202\n",
      "Epoch 28/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.6567 - recon_metric: 339.7321 - val_loss: 340.6135 - val_recon_metric: 339.6758\n",
      "Epoch 29/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.6208 - recon_metric: 339.7006 - val_loss: 340.5544 - val_recon_metric: 339.6272\n",
      "Epoch 30/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.5918 - recon_metric: 339.6757 - val_loss: 340.5216 - val_recon_metric: 339.5942\n",
      "Epoch 31/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.5495 - recon_metric: 339.6376 - val_loss: 340.5325 - val_recon_metric: 339.6116\n",
      "Epoch 32/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.5244 - recon_metric: 339.6159 - val_loss: 340.4449 - val_recon_metric: 339.5323\n",
      "Epoch 33/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4956 - recon_metric: 339.5884 - val_loss: 340.4859 - val_recon_metric: 339.5847\n",
      "Epoch 34/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.4719 - recon_metric: 339.5672 - val_loss: 340.4942 - val_recon_metric: 339.6067\n",
      "Epoch 35/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4489 - recon_metric: 339.5471 - val_loss: 340.4214 - val_recon_metric: 339.5262\n",
      "Epoch 36/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.4183 - recon_metric: 339.5199 - val_loss: 340.3767 - val_recon_metric: 339.4810\n",
      "Epoch 37/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4022 - recon_metric: 339.5056 - val_loss: 340.4024 - val_recon_metric: 339.5155\n",
      "Epoch 38/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3827 - recon_metric: 339.4868 - val_loss: 340.3543 - val_recon_metric: 339.4758\n",
      "Epoch 39/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3582 - recon_metric: 339.4651 - val_loss: 340.4132 - val_recon_metric: 339.5304\n",
      "Epoch 40/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3443 - recon_metric: 339.4536 - val_loss: 340.3165 - val_recon_metric: 339.4400\n",
      "Epoch 41/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.3276 - recon_metric: 339.4386 - val_loss: 340.3375 - val_recon_metric: 339.4563\n",
      "Epoch 42/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.3127 - recon_metric: 339.4256 - val_loss: 340.3096 - val_recon_metric: 339.4315\n",
      "Epoch 43/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2941 - recon_metric: 339.4085 - val_loss: 340.2976 - val_recon_metric: 339.4278\n",
      "Epoch 44/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.2848 - recon_metric: 339.4000 - val_loss: 340.2375 - val_recon_metric: 339.3611\n",
      "Epoch 45/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2618 - recon_metric: 339.3798 - val_loss: 340.2197 - val_recon_metric: 339.3425\n",
      "Epoch 46/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2486 - recon_metric: 339.3690 - val_loss: 340.2565 - val_recon_metric: 339.3657\n",
      "Epoch 47/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.2447 - recon_metric: 339.3647 - val_loss: 340.1922 - val_recon_metric: 339.3047\n",
      "Epoch 48/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.2348 - recon_metric: 339.3558 - val_loss: 340.2071 - val_recon_metric: 339.3239\n",
      "Epoch 49/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2118 - recon_metric: 339.3355 - val_loss: 340.2311 - val_recon_metric: 339.3384\n",
      "Epoch 50/300\n",
      "40000/40000 [==============================] - 2s 39us/step - loss: 340.2110 - recon_metric: 339.3348 - val_loss: 340.1655 - val_recon_metric: 339.2863\n",
      "Epoch 51/300\n",
      "40000/40000 [==============================] - 1s 37us/step - loss: 340.1906 - recon_metric: 339.3163 - val_loss: 340.1676 - val_recon_metric: 339.2799\n",
      "Epoch 52/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1828 - recon_metric: 339.3084 - val_loss: 340.3686 - val_recon_metric: 339.4613\n",
      "Epoch 53/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.1798 - recon_metric: 339.3053 - val_loss: 340.1637 - val_recon_metric: 339.3017\n",
      "Epoch 54/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1654 - recon_metric: 339.2927 - val_loss: 340.1332 - val_recon_metric: 339.2657\n",
      "Epoch 55/300\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 340.1568 - recon_metric: 339.2849 - val_loss: 340.1092 - val_recon_metric: 339.2457\n",
      "Epoch 56/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.1477 - recon_metric: 339.2756 - val_loss: 340.1935 - val_recon_metric: 339.3280\n",
      "Epoch 57/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1349 - recon_metric: 339.2633 - val_loss: 340.1038 - val_recon_metric: 339.2415\n",
      "Epoch 58/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.1320 - recon_metric: 339.2607 - val_loss: 340.1635 - val_recon_metric: 339.3000\n",
      "Epoch 59/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1141 - recon_metric: 339.2433 - val_loss: 340.0633 - val_recon_metric: 339.1990\n",
      "Epoch 60/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1124 - recon_metric: 339.2409 - val_loss: 340.0716 - val_recon_metric: 339.2027\n",
      "Epoch 61/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1013 - recon_metric: 339.2299 - val_loss: 340.1988 - val_recon_metric: 339.3348\n",
      "Epoch 62/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.1054 - recon_metric: 339.2336 - val_loss: 340.1020 - val_recon_metric: 339.2167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8a9ad30610>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting VAE #\n",
    "vae.fit([tr_train,log_sigma_input_train], tr_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([tr_test, log_sigma_input_test], tr_test),\n",
    "        callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65206206, -0.6052956 , -0.8986118 ],\n",
       "       [-0.65482724,  0.9320263 ,  1.2972934 ],\n",
       "       [-0.87104845, -2.4526317 ,  0.8720732 ],\n",
       "       ...,\n",
       "       [-3.4867055 ,  1.6833498 , -0.5314662 ],\n",
       "       [ 0.674997  , -1.3439927 ,  1.0985439 ],\n",
       "       [-0.06504131, -0.5549359 ,  2.3187962 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sigma_input = np.full((tr.shape[0], 1), z_log_sigma_prior)\n",
    "u_t_mean = encoder_z_mean.predict([tr, log_sigma_input], batch_size = batch_size)\n",
    "u_t_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(-0.16571916547719565, 9.999214617874603e-305)\n",
      "1\n",
      "(-0.5256800884366084, 0.0)\n",
      "2\n",
      "(-0.2116353047190557, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "u_pcs = PCA(n_components = latent_dim).fit_transform(u)\n",
    "uhat_pcs = PCA(n_components = latent_dim).fit_transform(u_t_mean)\n",
    "for i in range(latent_dim):\n",
    "    print(i)\n",
    "    print(pearsonr(u_pcs[:,i], uhat_pcs[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1277642]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the coef of imput_log_sigma #\n",
    "encoder_z_log_sigma.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22881219"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output sigma_u_t #\n",
    "u_log_sigma = encoder_z_log_sigma.predict([tr_train, log_sigma_input_train], batch_size = batch_size)\n",
    "# print(u_log_sigma)\n",
    "u_t_sigma = np.exp(u_log_sigma)\n",
    "u_t_sigma[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22881218301308015"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output sigma_u_t #\n",
    "np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling Estimates (ISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae ouput sigma_u_t #\n",
    "u_t_sigma = np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_t_u_ise(i):\n",
    "    if (i % 1000 == 0): \n",
    "        print(i)\n",
    "    nsim = 100\n",
    "    t = tr[i]\n",
    "    u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "    u_samples = np.random.multivariate_normal(mean = u_t_mean[0], \\\n",
    "                                cov = u_t_sigma*np.identity(latent_dim), size = nsim)\n",
    "    # f(t|u) #\n",
    "    p_t_u = generator.predict(u_samples) ## sim in rach row\n",
    "    f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "    # f(u) #\n",
    "    f_u = multivariate_normal(mean=[0]*latent_dim, cov=np.identity(latent_dim)).pdf(u_samples)\n",
    "    # q(u|t) #\n",
    "    q_u_t = multivariate_normal(mean = u_t_mean[0], cov = u_t_sigma*np.identity(latent_dim)).pdf(u_samples)\n",
    "    # w = f(t|u)f(u)/q(u|t) #\n",
    "    nsamples = nsim\n",
    "    w = 10**(np.log10(f_t_u)+np.log10(f_u)-np.log10(q_u_t)).to_numpy().reshape(nsamples,1)\n",
    "    weight = 10**(np.log10(w) - np.log10(w.mean()))\n",
    "    mu_u_t_ise = (u_samples*weight).mean(axis=0)\n",
    "    mu_uu_t_ise = (np.apply_along_axis(lambda x: np.outer(x,x),1,u_samples)*weight.reshape(nsamples,1,1)).mean(axis=0)\n",
    "    cov_u_t_ise = mu_uu_t_ise - np.outer(mu_u_t_ise, mu_u_t_ise)\n",
    "    #     mu_u2_t_ise = (u_samples**2 * weight).mean(axis=0)\n",
    "    #     var_u_t_ise = mu_u2_t_ise - mu_u_t_ise**2\n",
    "    result = np.array([u_t_mean[0], mu_u_t_ise, cov_u_t_ise, weight.reshape(len(w))], dtype=object)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "ise_results = [f_t_u_ise(i) for i in range(tr.shape[0])]\n",
    "# mu_u_t (VAE output); mu_u_t_ise, var_u_t_ise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_u_t_output = np.empty((0, latent_dim))\n",
    "mu_u_t_ise = np.empty((0, latent_dim))\n",
    "cov_u_t_ise = np.empty((0, latent_dim, latent_dim))\n",
    "for i in range(len(ise_results)):\n",
    "    mu_u_t_output = np.append(mu_u_t_output, ise_results[i][0].reshape(1,latent_dim), axis = 0)\n",
    "    mu_u_t_ise = np.append(mu_u_t_ise, ise_results[i][1].reshape(1,latent_dim), axis = 0)\n",
    "    cov_u_t_ise = np.append(cov_u_t_ise, ise_results[i][2].reshape(1,latent_dim,latent_dim), axis = 0)\n",
    "weight_mat = [ise_results[i][3] for i in range(len(ise_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26066986,  0.06580309,  0.26675362])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output E(u|t) #\n",
    "mu_u_t_output.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10913082,  0.03865301,  0.12847931])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE E(u|t) #\n",
    "mu_u_t_ise.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05235501509521128\n"
     ]
    }
   ],
   "source": [
    "# output Var(u|t) #\n",
    "print(u_t_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17545018,  0.0081635 , -0.00346365],\n",
       "       [ 0.0081635 ,  0.1743882 , -0.00179012],\n",
       "       [-0.00346365, -0.00179012,  0.1867698 ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE Cov(u|t) #\n",
    "cov_u_t_ise_ave = cov_u_t_ise.mean(axis=0)\n",
    "cov_u_t_ise_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mu_u_t_ise).to_csv('mu_u_t_ise.csv', index = False)\n",
    "pd.DataFrame(cov_u_t_ise_ave).to_csv('cov_u_t_ise.csv', index = False)\n",
    "# mu_u_t_ise = pd.read_csv('mu_u_t_ise.csv').to_numpy()\n",
    "# cov_u_t_ise = pd.read_csv('cov_u_t_ise.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95062671, -0.02078696,  0.01938876],\n",
       "       [-0.02078696,  0.94954662,  0.00707655],\n",
       "       [ 0.01938876,  0.00707655,  0.99638468]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "cov_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISE Cov(U|t):\n",
      "[[ 0.17545018  0.0081635  -0.00346365]\n",
      " [ 0.0081635   0.1743882  -0.00179012]\n",
      " [-0.00346365 -0.00179012  0.1867698 ]]\n",
      "Cov(U):\n",
      "[[ 0.95062671 -0.02078696  0.01938876]\n",
      " [-0.02078696  0.94954662  0.00707655]\n",
      " [ 0.01938876  0.00707655  0.99638468]]\n",
      "var(u|t)/var(u) for each dimension of U\n",
      "[0.18456264 0.18365417 0.18744749]\n"
     ]
    }
   ],
   "source": [
    "print('ISE Cov(U|t):')\n",
    "print(cov_u_t_ise_ave)\n",
    "# Cov(u) = E(Cov(u|t)) + Cov(E(u|t))\n",
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "print('Cov(U):')\n",
    "print(cov_u)\n",
    "print('var(u|t)/var(u) for each dimension of U')\n",
    "print(np.diag(cov_u_t_ise_ave)/np.diag(cov_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting LM with Y ~ T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lmfit_y_t = LinearRegression().fit(tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficientï¼š tau_naive #\n",
    "tau_naive = lmfit_y_t.coef_.reshape(k,1)  # lmfit_y_t.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.85284773,  -0.91089822,  12.09315041,  -3.25899139,\n",
       "        -5.4797407 ,   0.04407081,  -7.36560428,  -6.34271326,\n",
       "         1.02171412, -13.13578741,  -0.81916312,   6.95351223,\n",
       "        -6.64761181,   9.55408621,  -5.28890334,  -8.35829301,\n",
       "       -11.70187037,  -4.04250111,  -0.03879425, -11.70187037,\n",
       "        -1.34684483,  -0.07948535,  -4.37678675,   5.5480885 ,\n",
       "        -7.23402651,  -6.62003685,   4.65643001,  10.25216207,\n",
       "        -4.34226198,  -3.64125522,   0.11976007,  10.25216207,\n",
       "       -13.14796222,  -9.29432922,  -6.37545183,  -2.92180524,\n",
       "         1.2008204 ,  -3.36721287,  -8.44000453, -13.45396435,\n",
       "        10.40681793,  -1.7186441 ,  -7.6641055 ,   8.42386768,\n",
       "         5.18251881,  -3.52986793,  -6.43146727,   1.00636515,\n",
       "        -3.6737733 ,   8.76517495])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_naive[nontrivial_effect_index].reshape(50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03580907, -1.73027062, -0.83099135,  0.75735521, -0.91439045,\n",
       "       -1.19905462, -2.05937236, -1.89784424, -0.55249467, -2.39413616,\n",
       "        0.69515799,  2.11684109, -1.86499218,  1.02549347, -0.95280136,\n",
       "       -0.38364411, -2.20705234,  1.4174158 ,  0.16782274, -2.20705234,\n",
       "       -1.44307753, -1.0034953 , -1.19302549, -0.93953058,  0.48178681,\n",
       "        0.37904891,  0.62561858,  2.20776609, -1.10521291, -1.35561128,\n",
       "       -0.21709315,  2.20776609,  0.8889774 ,  0.09308352,  0.48711883,\n",
       "       -1.5722218 ,  1.79951197, -1.65310123, -1.68726251, -1.6717612 ,\n",
       "        1.81350111, -1.07284413,  1.3298569 ,  1.68364053,  1.03405283,\n",
       "        0.94847316, -0.23508562, -1.28451368, -0.07296996,  1.30025698])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_obs[nontrivial_effect_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lmfit_y_t.predict(tr)\n",
    "var_y_t = sum((y - y_hat)**2)/(y.shape[0]-1-tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5888.45788407])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by raw VAE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_u_t_diff_org(i):\n",
    "    print(i)\n",
    "    t1 = copy.deepcopy(tr)\n",
    "    t1[:,i] = 1\n",
    "    u_t1 = encoder_z_mean.predict([t1, log_sigma_input], batch_size = batch_size)\n",
    "    t0 = copy.deepcopy(tr)\n",
    "    t0[:,i] = 0\n",
    "    u_t0 = encoder_z_mean.predict([t0, log_sigma_input], batch_size = batch_size)\n",
    "    return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "u_t_diff_org_all = [cal_u_t_diff_org(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(u_t_diff_org_all).to_csv('u_t_diff_org_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by ISE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mu_u_t_ise(t):\n",
    "    nsim = 100\n",
    "    u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "    u_samples = np.random.normal(loc = u_t_mean[0,0], scale = u_t_sigma, size = nsim)\n",
    "    # f(t|u) #\n",
    "    p_t_u = generator.predict(u_samples.reshape(nsim,1)) ## sim in rach row\n",
    "    f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "    # f(u) #\n",
    "    f_u = norm(loc=0, scale=1).pdf(u_samples)\n",
    "    # q(u|t) #\n",
    "    q_u_t = norm(loc=u_t_mean[0,0], scale=u_t_sigma).pdf(u_samples)\n",
    "    # w = f(t|u)f(u)/q(u|t) #\n",
    "    w = f_t_u*f_u/q_u_t\n",
    "    mu_u_t_ise = (u_samples*w).mean() / w.mean()\n",
    "    return mu_u_t_ise ## averaged (single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_u_t_diff(i):\n",
    "    print(i)\n",
    "    t1 = copy.deepcopy(tr)\n",
    "    t1[:,i] = 1\n",
    "    u_t1 = np.apply_along_axis(cal_mu_u_t_ise, 1, t1)\n",
    "    t0 = copy.deepcopy(tr)\n",
    "    t0[:,i] = 0\n",
    "    u_t0 = np.apply_along_axis(cal_mu_u_t_ise, 1, t0)\n",
    "    return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_t_diff_ise_all = [cal_u_t_diff(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_t_diff_ise_t1 = cal_u_t_diff(0) \n",
    "u_t_diff_ise_t2 = cal_u_t_diff(1) \n",
    "u_t_diff_ise_t3 = cal_u_t_diff(2) \n",
    "u_t_diff_ise_t4 = cal_u_t_diff(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(u_t_diff_org_all)[:4])\n",
    "print(pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "                    u_t_diff_ise_t3, u_t_diff_ise_t4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(u_t_diff_all_ise).to_csv('u_t_diff_all_ise.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "#               u_t_diff_ise_t3, u_t_diff_ise_t4]).to_csv('u_t_diff_ise_1234.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
