{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## strong confoundedness: large B and gamma\n",
    "## small true effect: small tau\n",
    "\n",
    "k = 500\n",
    "s = 3\n",
    "np.random.seed(123)\n",
    "B = np.random.uniform(low=-2, high=2, size=(k,s))\n",
    "# B  = np.zeros(shape = (k,s))\n",
    "# B = np.array([2, 0.5, -0.4, 0.2]).reshape(k, s)\n",
    "sigma2_tstar = 100\n",
    "\n",
    "gamma = np.array([100]*s).reshape(s,1)\n",
    "tau = np.random.uniform(low=-0.1, high=0.1, size=(k,1))\n",
    "sigma2_y = 10\n",
    "\n",
    "# set some tau to large values #\n",
    "nontrivial_effect_index = np.random.randint(low = 0, high = k, size = int(k*0.1))\n",
    "nontrivial_effect_index = np.unique(nontrivial_effect_index)\n",
    "nontrivial_effect = np.random.uniform(low=-2, high=2, size=(nontrivial_effect_index.shape[0],1))\n",
    "tau[nontrivial_effect_index] = nontrivial_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "u = np.random.multivariate_normal(mean = np.full(s, 0), cov=np.identity(s), size=n)\n",
    "tr_star = np.dot(u, np.transpose(B)) + \\\n",
    "          np.random.multivariate_normal(mean = np.full(k, 0), cov=sigma2_tstar*np.identity(k), size = n)\n",
    "tr = np.where(tr_star > 0, 1, 0)\n",
    "y = np.dot(tr, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n).reshape(n,1)\n",
    "# y = np.dot(tr_star, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.54596421e+02+0.00000000e+00j,  6.94210362e+02+0.00000000e+00j,\n",
       "        6.06695658e+02+0.00000000e+00j, -1.18014492e-14+4.44430481e-14j,\n",
       "       -1.18014492e-14-4.44430481e-14j,  4.73855313e-14+0.00000000e+00j,\n",
       "        2.97170570e-14+3.71075988e-14j,  2.97170570e-14-3.71075988e-14j,\n",
       "       -2.04644728e-14+3.45300219e-14j, -2.04644728e-14-3.45300219e-14j])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the first eigenvalue of BB is large, thus var(u|t) is smaller given sigma2_star=100\n",
    "BB = np.dot(B, np.transpose(B))\n",
    "BB_w, BB_v = np.linalg.eig(BB) ## eigenvalues, eigevector\n",
    "BB_w[0:10] ## the first eigenvalue, 654.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cov(u|t*):\n",
      "[[ 0.12583743  0.01137063 -0.00124313]\n",
      " [ 0.01137063  0.13200804 -0.00338941]\n",
      " [-0.00124313 -0.00338941  0.12658369]]\n"
     ]
    }
   ],
   "source": [
    "## some theoretical values #\n",
    "coef_mu_u_tstar = np.dot(np.transpose(B), np.linalg.inv(np.dot(B, np.transpose(B)) + sigma2_tstar*np.identity(k)))\n",
    "cov_u_tstar = np.identity(s) - np.dot(coef_mu_u_tstar, B)\n",
    "print('Cov(u|t*):')\n",
    "print(cov_u_tstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. Var(y|t) [Var(y|t*)]: 3989.0533932258177\n",
      "confounding fraction in residual of Y: 0.9974931395962305\n"
     ]
    }
   ],
   "source": [
    "var_y_t = np.dot(np.dot(np.transpose(gamma), cov_u_tstar), gamma) + sigma2_y\n",
    "print('Approx. Var(y|t) [Var(y|t*)]: ' + str(var_y_t[0,0]))\n",
    "print('confounding fraction in residual of Y: ' + str(1 - sigma2_y/var_y_t[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.91359638e-03, -8.09280562e-02,  8.20469453e-02,  3.36036929e-02,\n",
       "        1.99419230e-03, -1.37468521e-02, -8.97068977e-02, -5.71411584e-02,\n",
       "        7.12158395e-02,  2.44408826e-02, -6.12249759e-02, -6.21593565e-02,\n",
       "       -2.99748975e-02,  2.30593077e-02,  8.48305194e-03,  5.40697504e-03,\n",
       "        7.37861600e-02,  7.22502913e-02,  6.90397006e-02,  4.04903368e-02,\n",
       "       -2.15099573e-02, -4.61311460e-02,  9.11385279e-02, -9.09430074e-02,\n",
       "        2.36766147e-02,  6.92248245e-02, -4.56174180e-02,  9.57492143e-02,\n",
       "        7.71847944e-02, -6.01343823e-02, -4.59381366e-02,  7.94605820e-02,\n",
       "       -7.81496413e-02, -6.23045657e-02, -1.16390905e+00, -7.05971310e-02,\n",
       "        1.45090342e-02,  1.51090426e-02,  1.92708801e-02, -4.98597218e-02,\n",
       "        6.42161734e-02,  9.40887244e-02, -4.44227955e-02,  7.30986638e-02,\n",
       "       -4.13895920e-02, -3.02145772e-02,  5.70886893e-02,  1.95423119e+00,\n",
       "       -8.76384854e-02, -2.26734800e-02, -7.40633559e-02, -8.40894489e-02,\n",
       "        1.77456040e-02, -6.32050509e-02,  8.34385077e-02,  4.58177133e-02,\n",
       "       -8.14008556e-02,  9.87936469e-02, -3.82486664e-02,  9.12585037e-03,\n",
       "        5.83698963e-02,  1.38186164e+00, -3.17672487e-02, -5.87462466e-02,\n",
       "       -2.42843376e-02,  4.69873345e-02,  4.28853788e-02, -6.35961643e-02,\n",
       "       -1.72425897e+00,  3.76413533e-02,  4.76299789e-02,  2.72073944e-02,\n",
       "       -6.10354576e-02,  7.36158111e-02, -4.23104452e-02, -2.60156394e-02,\n",
       "        1.16995604e-02,  7.18757010e-02,  2.99300624e-02,  4.03375409e-02,\n",
       "       -5.67263441e-02, -2.94876352e-02, -7.26213164e-02,  4.73806833e-02,\n",
       "        8.97515613e-02,  4.69736523e-02,  4.06989014e-03,  3.86730705e-02,\n",
       "       -1.33549618e-02,  2.27370747e-01, -1.76995231e+00,  4.48250581e-02,\n",
       "        2.44875939e-02,  7.46195747e-02, -3.49317811e-02,  3.33690181e-02,\n",
       "       -6.34391351e-03, -7.52100465e-02,  8.21394274e-03,  2.63818975e-02,\n",
       "        1.70914570e+00, -7.49456886e-02, -8.49433650e-02,  3.29676930e-02,\n",
       "       -1.71966913e+00,  8.48188091e-02, -2.23628293e-02,  8.76631669e-02,\n",
       "        6.39606944e-02, -1.51916663e-02, -1.87359934e-02, -3.33042395e-02,\n",
       "       -8.61766788e-02, -9.80547728e-02, -5.09002673e-02, -4.90765302e-02,\n",
       "        4.59322092e-02, -6.85258645e-02,  3.84843268e-02,  4.60670600e-02,\n",
       "       -8.05656782e-03, -7.64692991e-02,  4.39019224e-02,  8.27294503e-02,\n",
       "       -2.67641092e-02,  5.17479555e-02,  4.83837436e-02, -8.91915665e-02,\n",
       "       -8.88584669e-02, -1.02857860e+00, -1.47880466e+00,  8.41123807e-02,\n",
       "       -7.16673753e-02,  1.66780416e-02, -4.35893825e-02, -5.54598266e-02,\n",
       "        4.76777105e-02, -1.73616243e-02, -1.24615783e-02,  6.88802522e-02,\n",
       "       -5.22630510e-02,  5.66847555e-03,  6.55420761e-02, -7.36222393e-02,\n",
       "        1.92365943e-02,  6.74380175e-02,  5.28931085e-02,  7.68964221e-02,\n",
       "       -9.34871602e-02,  6.77919588e-01, -5.92126985e-02,  1.14037456e-02,\n",
       "        5.49557798e-02, -1.59617070e-02,  4.08407421e-02,  6.80821396e-02,\n",
       "        8.91437380e-02,  9.01862104e-03, -3.67800139e-02, -2.19947693e-02,\n",
       "       -1.04635961e+00, -5.19366739e-02,  7.66528801e-02, -8.84327513e-02,\n",
       "        9.42650997e-02, -7.61785554e-02,  7.66778113e-02,  8.15433561e-03,\n",
       "        1.22655495e-02,  9.61311908e-02,  9.02724220e-02,  5.97216293e-02,\n",
       "        3.03166895e-02,  3.55402773e-02, -7.33062944e-02, -6.51893451e-03,\n",
       "       -1.41668399e-02, -7.96181750e-02, -9.77343125e-02, -9.67463385e-02,\n",
       "        4.74933154e-02, -1.05087976e-02, -1.88507648e-03, -4.92886781e-02,\n",
       "        8.16766658e-03, -7.45585883e-02, -6.72900549e-02,  6.35288066e-02,\n",
       "       -9.45141294e-02,  8.32027025e-02, -3.23624152e-02,  8.64316779e-02,\n",
       "       -8.72602405e-02, -1.33897426e+00, -1.23126830e+00,  8.75609683e-02,\n",
       "       -8.19992933e-02,  6.18386090e-02,  2.26047843e-02,  2.80016109e-02,\n",
       "        9.84618203e-02, -3.57834372e-02, -2.79624058e-02,  7.17077505e-03,\n",
       "        2.50167696e-02, -3.76507760e-02,  4.12647072e-02, -8.35750802e-02,\n",
       "       -6.10390430e-02, -9.73496278e-02,  4.99240224e-02,  8.95796209e-02,\n",
       "        5.37422004e-02, -6.54257771e-02, -4.32284274e-02,  3.42356556e-02,\n",
       "       -3.81106751e-02, -1.86363087e-02, -4.88392070e-02,  9.16085157e-01,\n",
       "        7.38952582e-02, -5.20614339e-02,  5.90744663e-03,  3.30007298e-02,\n",
       "        8.72452790e-01, -4.94054476e-02,  1.34534845e-02,  6.50399009e-02,\n",
       "       -5.44077046e-02,  7.97695473e-02, -1.65303916e+00,  7.09157545e-01,\n",
       "        6.26949873e-02,  6.98497149e-02, -4.36679556e-02, -7.69697339e-02,\n",
       "       -9.89267076e-01,  7.61500266e-02, -1.11955695e-03,  7.87771525e-01,\n",
       "        1.43476655e+00,  5.60504332e-02,  6.48426375e-02,  7.85071420e-02,\n",
       "        2.83539626e-02,  7.84240807e-02,  7.33706206e-02, -4.56712543e-02,\n",
       "        1.77095769e-02, -4.75682549e-02,  2.75638315e-02,  7.94295036e-02,\n",
       "       -3.37207958e-03,  6.49120340e-02, -9.65790943e-02, -5.91467529e-01,\n",
       "        8.88406328e-02, -7.82204373e-01,  4.19291025e-02,  6.38461877e-02,\n",
       "       -7.51464063e-02, -1.71108136e-02, -5.86833187e-02,  4.93117795e-02,\n",
       "       -1.88170345e-02, -9.99728116e-02, -4.74732674e-02, -5.93395100e-02,\n",
       "       -8.50463504e-02,  1.91208518e+00,  7.75386579e-01, -5.62137202e-02,\n",
       "       -1.54610940e+00, -3.52443746e-02, -7.44594839e-02,  4.63566421e-03,\n",
       "        7.84672323e-02,  5.55915652e-02, -7.03621568e-02, -4.02038930e-02,\n",
       "       -5.25662727e-02,  8.55271991e-02, -4.41426511e-02, -6.61236414e-02,\n",
       "       -8.97656812e-03, -1.46567845e+00, -6.82975538e-02, -9.50978789e-02,\n",
       "       -8.10469968e-02,  3.01895466e-02, -6.73103285e-02, -4.82313160e-02,\n",
       "        3.51437497e-02,  7.42724206e-02,  9.05343442e-02, -2.32834411e-02,\n",
       "       -5.47257056e-02,  4.78264548e-02,  4.24847817e-02,  1.89085774e+00,\n",
       "       -8.04223929e-02,  7.56288697e-02, -6.48679519e-02,  4.94670790e-02,\n",
       "        9.90850595e-02, -5.36443114e-02,  4.84359591e-02,  5.48315047e-02,\n",
       "       -4.38081265e-02,  8.42460943e-02,  8.33022440e-01, -5.42240081e-02,\n",
       "       -4.10841192e-02, -8.19893123e-02, -5.09495631e-02,  5.74326780e-02,\n",
       "        1.63241663e-03,  1.32314094e-03, -1.26953849e+00,  1.83374207e-02,\n",
       "        2.21740698e-02, -5.63478259e-02, -8.50697455e-02,  4.03569432e-02,\n",
       "        2.66872570e-02, -3.54997141e-03,  7.90845162e-02,  9.54538751e-02,\n",
       "        4.64788264e-02,  9.24546717e-02, -7.05710781e-02, -6.80908583e-02,\n",
       "        1.64104514e+00, -2.46923330e-02,  1.18920196e-02,  8.89464427e-02,\n",
       "        2.07949466e-02,  7.40083277e-02,  4.14592834e-02,  7.11753796e-02,\n",
       "       -5.94058250e-03,  8.98760454e-02, -7.49477663e-02,  6.15342281e-02,\n",
       "       -5.29415549e-02,  2.37628186e-02,  5.52524443e-02,  1.45638393e-01,\n",
       "        4.52471820e-01, -9.89422038e-01, -6.76341380e-02,  9.20887129e-01,\n",
       "       -4.93639109e-02,  3.75928533e-02, -6.18757224e-02, -9.04437991e-02,\n",
       "        5.69584747e-02,  6.34892510e-02, -6.75140322e-02,  6.14199078e-02,\n",
       "       -3.43915917e-02,  1.42294817e-02,  2.03764029e-02,  8.27264822e-02,\n",
       "        1.40461041e-02,  7.37913015e-02, -1.56959435e-02,  3.06344620e-02,\n",
       "        1.48641395e+00,  4.24428609e-02,  8.55509152e-02, -1.45647261e-02,\n",
       "        2.94579094e-02,  8.85959470e-03, -9.70998199e-02,  8.15565916e-02,\n",
       "        6.18580681e-02, -1.69030474e-02, -8.18063407e-02, -2.96809318e-02,\n",
       "        5.12836351e-03, -5.11653273e-02,  6.65010184e-02, -4.09045937e-02,\n",
       "       -5.90133619e-02, -6.54271991e-02,  4.52539183e-02,  9.32453729e-03,\n",
       "        3.91948798e-02, -4.19309791e-02, -1.94806865e+00,  7.91214546e-02,\n",
       "       -3.39195080e-02,  8.05898273e-02,  9.32570881e-02, -9.60931852e-02,\n",
       "       -1.15391498e-03,  2.44112007e-02, -1.22224590e+00, -2.99671505e-02,\n",
       "       -9.43912783e-02,  2.32982952e-02, -7.36833951e-03, -4.74170021e-02,\n",
       "        5.55180553e-02,  8.58288178e-03, -3.35145533e-02,  8.70605814e-02,\n",
       "        9.51668730e-02, -6.35197882e-02,  9.56783590e-02,  1.75208037e-02,\n",
       "        7.50716865e-02, -4.17004948e-02, -9.06784521e-02, -9.74829837e-02,\n",
       "       -1.38137830e-02,  5.09956410e-01, -1.09037146e-02,  4.87940495e-02,\n",
       "        3.36668504e-02,  4.63910996e-02,  1.89161756e-02,  9.07979628e-02,\n",
       "       -5.19794661e-02,  8.07653017e-02,  9.63457440e-02, -9.92903859e-02,\n",
       "       -1.37724428e-02,  3.52144422e-02,  2.29071101e-02, -1.36082929e+00,\n",
       "        3.20736164e-02, -2.30543743e-01,  8.52658639e-02,  8.04228865e-02,\n",
       "        6.68371860e-02, -1.85250640e-02,  3.94996403e-02,  9.11999786e-02,\n",
       "       -5.20227896e-02, -2.40375321e-03,  5.15142373e-02, -1.80573921e+00,\n",
       "        5.67496168e-02, -4.71979102e-02, -7.38349117e-02, -5.75747632e-03,\n",
       "        3.29629208e-02,  1.95092819e-02,  2.21350957e-02, -8.14676267e-02,\n",
       "       -1.73624173e-03, -5.95069930e-02,  4.21356093e-02,  1.16838335e-02,\n",
       "        2.01555104e-02,  1.91133790e-02, -3.87573042e-02, -8.33261560e-02,\n",
       "       -1.73821872e-02, -4.04517172e-02,  1.39414978e-02, -5.53724503e-02,\n",
       "       -5.52596405e-02, -3.49688019e-02, -6.53351014e-02, -1.95978478e-02,\n",
       "        6.48034402e-02,  7.04008768e-02, -1.41772389e+00, -3.38539718e-02,\n",
       "        2.26424170e-02, -5.46102989e-02,  8.38953819e-02,  4.13266740e-02,\n",
       "       -5.59420122e-02, -7.77059812e-01, -8.52448285e-02, -6.24051844e-02,\n",
       "       -2.08904236e-02,  5.01577292e-02, -7.18002104e-02, -9.27075582e-02,\n",
       "        7.83401150e-02, -3.29248949e-02, -3.39276796e-02,  5.02148241e-03,\n",
       "       -1.60340541e-02, -4.09539750e-02,  4.31158641e-02,  5.84337202e-02,\n",
       "        6.52559345e-02,  5.40060553e-02,  3.80855627e-02,  1.11250327e+00,\n",
       "        4.63300781e-02, -4.50577236e-02,  4.43635046e-02, -8.04274194e-02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True Treatment Effect\n",
    "tau.reshape(k,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect bias range:\n",
      "-0.7511657117030033\n",
      "0.7201117743876483\n",
      "obs effect range:\n",
      "-2.394136160637157\n",
      "2.2077660918815347\n"
     ]
    }
   ],
   "source": [
    "# Approx. Obs. Effect #\n",
    "effect_bias = np.dot(np.dot(np.transpose(gamma), coef_mu_u_tstar), \\\n",
    "                     np.identity(k)).reshape(k,)\n",
    "print('effect bias range:')\n",
    "print(effect_bias.min())\n",
    "print(effect_bias.max())\n",
    "effect_obs = tau.reshape(k,) + effect_bias\n",
    "print('obs effect range:')\n",
    "print(effect_obs.min())\n",
    "print(effect_obs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Bias for Nontrivial Ones:     [-0.19156168 -0.00601164  0.50798291 -0.16353192 -0.13218608  0.07048387\n",
      " -0.25363315 -0.17817511  0.03897286 -0.44606751 -0.08022859  0.1626099\n",
      " -0.21195302  0.31633593 -0.17574154 -0.387714   -0.43710003 -0.22362933\n",
      "  0.02218435 -0.43710003  0.02260092  0.0250833  -0.20360345  0.29173772\n",
      " -0.35123563 -0.29887068  0.17314676  0.29568091 -0.11594583 -0.13336539\n",
      "  0.01345059  0.29568091 -0.59743655 -0.41687289 -0.30065269 -0.09341714\n",
      " -0.09134576 -0.10699182 -0.32643323 -0.50785215  0.37873456 -0.02648453\n",
      " -0.3792888   0.30177889  0.16160004 -0.16403011 -0.23671804  0.1332102\n",
      " -0.15640847  0.38417182]\n",
      "Observed Effect for Nontrivial Ones: [ 0.03580907 -1.73027062 -0.83099135  0.75735521 -0.91439045 -1.19905462\n",
      " -2.05937236 -1.89784424 -0.55249467 -2.39413616  0.69515799  2.11684109\n",
      " -1.86499218  1.02549347 -0.95280136 -0.38364411 -2.20705234  1.4174158\n",
      "  0.16782274 -2.20705234 -1.44307753 -1.0034953  -1.19302549 -0.93953058\n",
      "  0.48178681  0.37904891  0.62561858  2.20776609 -1.10521291 -1.35561128\n",
      " -0.21709315  2.20776609  0.8889774   0.09308352  0.48711883 -1.5722218\n",
      "  1.79951197 -1.65310123 -1.68726251 -1.6717612   1.81350111 -1.07284413\n",
      "  1.3298569   1.68364053  1.03405283  0.94847316 -0.23508562 -1.28451368\n",
      " -0.07296996  1.30025698]\n",
      "True Effect for Nontrivial Ones:     [ 0.22737075 -1.72425897 -1.33897426  0.92088713 -0.78220437 -1.26953849\n",
      " -1.80573921 -1.71966913 -0.59146753 -1.94806865  0.77538658  1.95423119\n",
      " -1.65303916  0.70915755 -0.77705981  0.00406989 -1.76995231  1.64104514\n",
      "  0.14563839 -1.76995231 -1.46567845 -1.0285786  -0.98942204 -1.2312683\n",
      "  0.83302244  0.67791959  0.45247182  1.91208518 -0.98926708 -1.2222459\n",
      " -0.23054374  1.91208518  1.48641395  0.50995641  0.78777153 -1.47880466\n",
      "  1.89085774 -1.5461094  -1.36082929 -1.16390905  1.43476655 -1.04635961\n",
      "  1.7091457   1.38186164  0.87245279  1.11250327  0.00163242 -1.41772389\n",
      "  0.08343851  0.91608516]\n"
     ]
    }
   ],
   "source": [
    "print('Effect Bias for Nontrivial Ones:     ' + str(effect_bias[nontrivial_effect_index]))\n",
    "print('Observed Effect for Nontrivial Ones: ' + str(effect_obs[nontrivial_effect_index]))\n",
    "print('True Effect for Nontrivial Ones:     ' + str(tau[nontrivial_effect_index].reshape(50,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(tau).to_csv('tau.csv', index = False)\n",
    "# pd.DataFrame(u).to_csv('u.csv', index = False)\n",
    "# pd.DataFrame(tr).to_csv('tr.csv', index = False)\n",
    "# pd.DataFrame(y).to_csv('y.csv', index = False)\n",
    "# pd.DataFrame(nontrivial_effect_index).to_csv('nontrivial_effect_index.csv', index = False)\n",
    "n = 50000\n",
    "u = pd.read_csv('u.csv').to_numpy()\n",
    "tr = pd.read_csv('tr.csv').to_numpy()\n",
    "y = pd.read_csv('y.csv').to_numpy()\n",
    "tau = pd.read_csv('tau.csv').to_numpy()\n",
    "nontrivial_effect_index = pd.read_csv('nontrivial_effect_index.csv').to_numpy().reshape(50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting/LatentDim3')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "original_dim = k\n",
    "latent_dim = s\n",
    "intermediate_dim = 10\n",
    "epochs = 300\n",
    "epsilon_std = 1\n",
    "z_log_sigma_prior = np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder #\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim)(x)\n",
    "h = LeakyReLU(alpha = 0.1)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "# z_log_sigma = Dense(latent_dim)(h)\n",
    "## log_z_scale\n",
    "\n",
    "\n",
    "z_log_sigma_input = Input(batch_shape = (batch_size, 1))\n",
    "z_log_sigma = Dense(units = 1,  activation = \"linear\",\n",
    "                    kernel_initializer=initializers.Ones(),\n",
    "                    use_bias = False)(z_log_sigma_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from latent space #\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "# z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder #\n",
    "decoder_h1 = Dense(intermediate_dim)\n",
    "decoder_h2 = LeakyReLU(alpha = 0.1)\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h1(z)\n",
    "h_decoded = decoder_h2(h_decoded)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model([x, z_log_sigma_input], x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder_z_mean = Model([x, z_log_sigma_input], z_mean)\n",
    "encoder_z_log_sigma = Model([x, z_log_sigma_input], z_log_sigma)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h1(decoder_input)\n",
    "_h_decoded = decoder_h2(_h_decoded)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_metric(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    return xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.RMSprop(learning_rate=0.0005)\n",
    "# opt = optimizers.Adam(learning_rate=0.001)\n",
    "vae.compile(optimizer=opt, loss=vae_loss, metrics = [recon_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tr_train, tr_test = train_test_split(tr,train_size=0.8)\n",
    "log_sigma_input_train = np.full((tr_train.shape[0], 1), z_log_sigma_prior)\n",
    "log_sigma_input_test = np.full((tr_test.shape[0], 1), z_log_sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "40000/40000 [==============================] - 2s 49us/step - loss: 346.8502 - recon_metric: 346.6636 - val_loss: 346.7291 - val_recon_metric: 346.6055\n",
      "Epoch 2/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 346.6751 - recon_metric: 346.5442 - val_loss: 346.5910 - val_recon_metric: 346.4373\n",
      "Epoch 3/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 346.4131 - recon_metric: 346.1924 - val_loss: 346.1993 - val_recon_metric: 345.9050\n",
      "Epoch 4/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 345.8714 - recon_metric: 345.4674 - val_loss: 345.5515 - val_recon_metric: 345.0258\n",
      "Epoch 5/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 345.1980 - recon_metric: 344.6090 - val_loss: 344.8254 - val_recon_metric: 344.1248\n",
      "Epoch 6/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 344.4634 - recon_metric: 343.7009 - val_loss: 344.0968 - val_recon_metric: 343.2914\n",
      "Epoch 7/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 343.8077 - recon_metric: 342.9164 - val_loss: 343.4950 - val_recon_metric: 342.5497\n",
      "Epoch 8/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 343.2740 - recon_metric: 342.3012 - val_loss: 343.0107 - val_recon_metric: 342.0373\n",
      "Epoch 9/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 342.8328 - recon_metric: 341.8095 - val_loss: 342.6225 - val_recon_metric: 341.6152\n",
      "Epoch 10/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 342.4786 - recon_metric: 341.4210 - val_loss: 342.2644 - val_recon_metric: 341.2173\n",
      "Epoch 11/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 342.1807 - recon_metric: 341.1037 - val_loss: 342.0295 - val_recon_metric: 340.9741\n",
      "Epoch 12/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.9636 - recon_metric: 340.8782 - val_loss: 341.8605 - val_recon_metric: 340.7400\n",
      "Epoch 13/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 341.7813 - recon_metric: 340.6967 - val_loss: 341.7358 - val_recon_metric: 340.6204\n",
      "Epoch 14/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 341.6332 - recon_metric: 340.5593 - val_loss: 341.5295 - val_recon_metric: 340.4532\n",
      "Epoch 15/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 341.5006 - recon_metric: 340.4410 - val_loss: 341.4504 - val_recon_metric: 340.3800\n",
      "Epoch 16/300\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 341.3925 - recon_metric: 340.3486 - val_loss: 341.3449 - val_recon_metric: 340.2778\n",
      "Epoch 17/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.2975 - recon_metric: 340.2683 - val_loss: 341.2124 - val_recon_metric: 340.1844\n",
      "Epoch 18/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 341.2030 - recon_metric: 340.1900 - val_loss: 341.1526 - val_recon_metric: 340.1222\n",
      "Epoch 19/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 341.1317 - recon_metric: 340.1312 - val_loss: 341.0329 - val_recon_metric: 340.0303\n",
      "Epoch 20/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 341.0536 - recon_metric: 340.0666 - val_loss: 340.9855 - val_recon_metric: 339.9901\n",
      "Epoch 21/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.9951 - recon_metric: 340.0194 - val_loss: 340.9512 - val_recon_metric: 339.9488\n",
      "Epoch 22/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.9288 - recon_metric: 339.9617 - val_loss: 340.8872 - val_recon_metric: 339.9062\n",
      "Epoch 23/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.8861 - recon_metric: 339.9278 - val_loss: 340.8342 - val_recon_metric: 339.8749\n",
      "Epoch 24/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.8278 - recon_metric: 339.8782 - val_loss: 340.7716 - val_recon_metric: 339.8039\n",
      "Epoch 25/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.7791 - recon_metric: 339.8371 - val_loss: 340.7641 - val_recon_metric: 339.8080\n",
      "Epoch 26/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.7360 - recon_metric: 339.8000 - val_loss: 340.6668 - val_recon_metric: 339.7191\n",
      "Epoch 27/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.6925 - recon_metric: 339.7633 - val_loss: 340.6731 - val_recon_metric: 339.7202\n",
      "Epoch 28/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.6567 - recon_metric: 339.7321 - val_loss: 340.6135 - val_recon_metric: 339.6758\n",
      "Epoch 29/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.6208 - recon_metric: 339.7006 - val_loss: 340.5544 - val_recon_metric: 339.6272\n",
      "Epoch 30/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.5918 - recon_metric: 339.6757 - val_loss: 340.5216 - val_recon_metric: 339.5942\n",
      "Epoch 31/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.5495 - recon_metric: 339.6376 - val_loss: 340.5325 - val_recon_metric: 339.6116\n",
      "Epoch 32/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.5244 - recon_metric: 339.6159 - val_loss: 340.4449 - val_recon_metric: 339.5323\n",
      "Epoch 33/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4956 - recon_metric: 339.5884 - val_loss: 340.4859 - val_recon_metric: 339.5847\n",
      "Epoch 34/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.4719 - recon_metric: 339.5672 - val_loss: 340.4942 - val_recon_metric: 339.6067\n",
      "Epoch 35/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4489 - recon_metric: 339.5471 - val_loss: 340.4214 - val_recon_metric: 339.5262\n",
      "Epoch 36/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.4183 - recon_metric: 339.5199 - val_loss: 340.3767 - val_recon_metric: 339.4810\n",
      "Epoch 37/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.4022 - recon_metric: 339.5056 - val_loss: 340.4024 - val_recon_metric: 339.5155\n",
      "Epoch 38/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3827 - recon_metric: 339.4868 - val_loss: 340.3543 - val_recon_metric: 339.4758\n",
      "Epoch 39/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3582 - recon_metric: 339.4651 - val_loss: 340.4132 - val_recon_metric: 339.5304\n",
      "Epoch 40/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.3443 - recon_metric: 339.4536 - val_loss: 340.3165 - val_recon_metric: 339.4400\n",
      "Epoch 41/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.3276 - recon_metric: 339.4386 - val_loss: 340.3375 - val_recon_metric: 339.4563\n",
      "Epoch 42/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.3127 - recon_metric: 339.4256 - val_loss: 340.3096 - val_recon_metric: 339.4315\n",
      "Epoch 43/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2941 - recon_metric: 339.4085 - val_loss: 340.2976 - val_recon_metric: 339.4278\n",
      "Epoch 44/300\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 340.2848 - recon_metric: 339.4000 - val_loss: 340.2375 - val_recon_metric: 339.3611\n",
      "Epoch 45/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2618 - recon_metric: 339.3798 - val_loss: 340.2197 - val_recon_metric: 339.3425\n",
      "Epoch 46/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2486 - recon_metric: 339.3690 - val_loss: 340.2565 - val_recon_metric: 339.3657\n",
      "Epoch 47/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.2447 - recon_metric: 339.3647 - val_loss: 340.1922 - val_recon_metric: 339.3047\n",
      "Epoch 48/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.2348 - recon_metric: 339.3558 - val_loss: 340.2071 - val_recon_metric: 339.3239\n",
      "Epoch 49/300\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 340.2118 - recon_metric: 339.3355 - val_loss: 340.2311 - val_recon_metric: 339.3384\n",
      "Epoch 50/300\n",
      "40000/40000 [==============================] - 2s 39us/step - loss: 340.2110 - recon_metric: 339.3348 - val_loss: 340.1655 - val_recon_metric: 339.2863\n",
      "Epoch 51/300\n",
      "40000/40000 [==============================] - 1s 37us/step - loss: 340.1906 - recon_metric: 339.3163 - val_loss: 340.1676 - val_recon_metric: 339.2799\n",
      "Epoch 52/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1828 - recon_metric: 339.3084 - val_loss: 340.3686 - val_recon_metric: 339.4613\n",
      "Epoch 53/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.1798 - recon_metric: 339.3053 - val_loss: 340.1637 - val_recon_metric: 339.3017\n",
      "Epoch 54/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1654 - recon_metric: 339.2927 - val_loss: 340.1332 - val_recon_metric: 339.2657\n",
      "Epoch 55/300\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 340.1568 - recon_metric: 339.2849 - val_loss: 340.1092 - val_recon_metric: 339.2457\n",
      "Epoch 56/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.1477 - recon_metric: 339.2756 - val_loss: 340.1935 - val_recon_metric: 339.3280\n",
      "Epoch 57/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1349 - recon_metric: 339.2633 - val_loss: 340.1038 - val_recon_metric: 339.2415\n",
      "Epoch 58/300\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 340.1320 - recon_metric: 339.2607 - val_loss: 340.1635 - val_recon_metric: 339.3000\n",
      "Epoch 59/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1141 - recon_metric: 339.2433 - val_loss: 340.0633 - val_recon_metric: 339.1990\n",
      "Epoch 60/300\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 340.1124 - recon_metric: 339.2409 - val_loss: 340.0716 - val_recon_metric: 339.2027\n",
      "Epoch 61/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 340.1013 - recon_metric: 339.2299 - val_loss: 340.1988 - val_recon_metric: 339.3348\n",
      "Epoch 62/300\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 340.1054 - recon_metric: 339.2336 - val_loss: 340.1020 - val_recon_metric: 339.2167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8a9ad30610>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting VAE #\n",
    "vae.fit([tr_train,log_sigma_input_train], tr_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([tr_test, log_sigma_input_test], tr_test),\n",
    "        callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65206206, -0.6052956 , -0.8986118 ],\n",
       "       [-0.65482724,  0.9320263 ,  1.2972934 ],\n",
       "       [-0.87104845, -2.4526317 ,  0.8720732 ],\n",
       "       ...,\n",
       "       [-3.4867055 ,  1.6833498 , -0.5314662 ],\n",
       "       [ 0.674997  , -1.3439927 ,  1.0985439 ],\n",
       "       [-0.06504131, -0.5549359 ,  2.3187962 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sigma_input = np.full((tr.shape[0], 1), z_log_sigma_prior)\n",
    "u_t_mean = encoder_z_mean.predict([tr, log_sigma_input], batch_size = batch_size)\n",
    "u_t_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(-0.16571916547719565, 9.999214617874603e-305)\n",
      "1\n",
      "(-0.5256800884366084, 0.0)\n",
      "2\n",
      "(-0.2116353047190557, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "u_pcs = PCA(n_components = latent_dim).fit_transform(u)\n",
    "uhat_pcs = PCA(n_components = latent_dim).fit_transform(u_t_mean)\n",
    "for i in range(latent_dim):\n",
    "    print(i)\n",
    "    print(pearsonr(u_pcs[:,i], uhat_pcs[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1277642]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the coef of imput_log_sigma #\n",
    "encoder_z_log_sigma.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22881219"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output sigma_u_t #\n",
    "u_log_sigma = encoder_z_log_sigma.predict([tr_train, log_sigma_input_train], batch_size = batch_size)\n",
    "# print(u_log_sigma)\n",
    "u_t_sigma = np.exp(u_log_sigma)\n",
    "u_t_sigma[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22881218301308015"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output sigma_u_t #\n",
    "np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling Estimates (ISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae ouput sigma_u_t #\n",
    "u_t_sigma = np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_t_u_ise(i):\n",
    "    if (i % 1000 == 0): \n",
    "        print(i)\n",
    "    nsim = 100\n",
    "    t = tr[i]\n",
    "    u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "    u_samples = np.random.multivariate_normal(mean = u_t_mean[0], \\\n",
    "                                cov = u_t_sigma*np.identity(latent_dim), size = nsim)\n",
    "    # f(t|u) #\n",
    "    p_t_u = generator.predict(u_samples) ## sim in rach row\n",
    "    f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "    # f(u) #\n",
    "    f_u = multivariate_normal(mean=[0]*latent_dim, cov=np.identity(latent_dim)).pdf(u_samples)\n",
    "    # q(u|t) #\n",
    "    q_u_t = multivariate_normal(mean = u_t_mean[0], cov = u_t_sigma*np.identity(latent_dim)).pdf(u_samples)\n",
    "    # w = f(t|u)f(u)/q(u|t) #\n",
    "    nsamples = nsim\n",
    "    w = 10**(np.log10(f_t_u)+np.log10(f_u)-np.log10(q_u_t)).to_numpy().reshape(nsamples,1)\n",
    "    weight = 10**(np.log10(w) - np.log10(w.mean()))\n",
    "    mu_u_t_ise = (u_samples*weight).mean(axis=0)\n",
    "    mu_uu_t_ise = (np.apply_along_axis(lambda x: np.outer(x,x),1,u_samples)*weight.reshape(nsamples,1,1)).mean(axis=0)\n",
    "    cov_u_t_ise = mu_uu_t_ise - np.outer(mu_u_t_ise, mu_u_t_ise)\n",
    "    #     mu_u2_t_ise = (u_samples**2 * weight).mean(axis=0)\n",
    "    #     var_u_t_ise = mu_u2_t_ise - mu_u_t_ise**2\n",
    "    result = np.array([u_t_mean[0], mu_u_t_ise, cov_u_t_ise, weight.reshape(len(w))], dtype=object)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "ise_results = [f_t_u_ise(i) for i in range(tr.shape[0])]\n",
    "# mu_u_t (VAE output); mu_u_t_ise, var_u_t_ise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_u_t_output = np.empty((0, latent_dim))\n",
    "mu_u_t_ise = np.empty((0, latent_dim))\n",
    "cov_u_t_ise = np.empty((0, latent_dim, latent_dim))\n",
    "for i in range(len(ise_results)):\n",
    "    mu_u_t_output = np.append(mu_u_t_output, ise_results[i][0].reshape(1,latent_dim), axis = 0)\n",
    "    mu_u_t_ise = np.append(mu_u_t_ise, ise_results[i][1].reshape(1,latent_dim), axis = 0)\n",
    "    cov_u_t_ise = np.append(cov_u_t_ise, ise_results[i][2].reshape(1,latent_dim,latent_dim), axis = 0)\n",
    "weight_mat = [ise_results[i][3] for i in range(len(ise_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26066986,  0.06580309,  0.26675362])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output E(u|t) #\n",
    "mu_u_t_output.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10913082,  0.03865301,  0.12847931])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE E(u|t) #\n",
    "mu_u_t_ise.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05235501509521128\n"
     ]
    }
   ],
   "source": [
    "# output Var(u|t) #\n",
    "print(u_t_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17545018,  0.0081635 , -0.00346365],\n",
       "       [ 0.0081635 ,  0.1743882 , -0.00179012],\n",
       "       [-0.00346365, -0.00179012,  0.1867698 ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE Cov(u|t) #\n",
    "cov_u_t_ise_ave = cov_u_t_ise.mean(axis=0)\n",
    "cov_u_t_ise_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mu_u_t_ise).to_csv('mu_u_t_ise.csv', index = False)\n",
    "pd.DataFrame(cov_u_t_ise_ave).to_csv('cov_u_t_ise.csv', index = False)\n",
    "# mu_u_t_ise = pd.read_csv('mu_u_t_ise.csv').to_numpy()\n",
    "# cov_u_t_ise = pd.read_csv('cov_u_t_ise.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95062671, -0.02078696,  0.01938876],\n",
       "       [-0.02078696,  0.94954662,  0.00707655],\n",
       "       [ 0.01938876,  0.00707655,  0.99638468]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "cov_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISE Cov(U|t):\n",
      "[[ 0.17545018  0.0081635  -0.00346365]\n",
      " [ 0.0081635   0.1743882  -0.00179012]\n",
      " [-0.00346365 -0.00179012  0.1867698 ]]\n",
      "Cov(U):\n",
      "[[ 0.95062671 -0.02078696  0.01938876]\n",
      " [-0.02078696  0.94954662  0.00707655]\n",
      " [ 0.01938876  0.00707655  0.99638468]]\n",
      "var(u|t)/var(u) for each dimension of U\n",
      "[0.18456264 0.18365417 0.18744749]\n"
     ]
    }
   ],
   "source": [
    "print('ISE Cov(U|t):')\n",
    "print(cov_u_t_ise_ave)\n",
    "# Cov(u) = E(Cov(u|t)) + Cov(E(u|t))\n",
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "print('Cov(U):')\n",
    "print(cov_u)\n",
    "print('var(u|t)/var(u) for each dimension of U')\n",
    "print(np.diag(cov_u_t_ise_ave)/np.diag(cov_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting LM with Y ~ T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lmfit_y_t = LinearRegression().fit(tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient： tau_naive #\n",
    "tau_naive = lmfit_y_t.coef_.reshape(k,1)  # lmfit_y_t.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.85284773,  -0.91089822,  12.09315041,  -3.25899139,\n",
       "        -5.4797407 ,   0.04407081,  -7.36560428,  -6.34271326,\n",
       "         1.02171412, -13.13578741,  -0.81916312,   6.95351223,\n",
       "        -6.64761181,   9.55408621,  -5.28890334,  -8.35829301,\n",
       "       -11.70187037,  -4.04250111,  -0.03879425, -11.70187037,\n",
       "        -1.34684483,  -0.07948535,  -4.37678675,   5.5480885 ,\n",
       "        -7.23402651,  -6.62003685,   4.65643001,  10.25216207,\n",
       "        -4.34226198,  -3.64125522,   0.11976007,  10.25216207,\n",
       "       -13.14796222,  -9.29432922,  -6.37545183,  -2.92180524,\n",
       "         1.2008204 ,  -3.36721287,  -8.44000453, -13.45396435,\n",
       "        10.40681793,  -1.7186441 ,  -7.6641055 ,   8.42386768,\n",
       "         5.18251881,  -3.52986793,  -6.43146727,   1.00636515,\n",
       "        -3.6737733 ,   8.76517495])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_naive[nontrivial_effect_index].reshape(50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03580907, -1.73027062, -0.83099135,  0.75735521, -0.91439045,\n",
       "       -1.19905462, -2.05937236, -1.89784424, -0.55249467, -2.39413616,\n",
       "        0.69515799,  2.11684109, -1.86499218,  1.02549347, -0.95280136,\n",
       "       -0.38364411, -2.20705234,  1.4174158 ,  0.16782274, -2.20705234,\n",
       "       -1.44307753, -1.0034953 , -1.19302549, -0.93953058,  0.48178681,\n",
       "        0.37904891,  0.62561858,  2.20776609, -1.10521291, -1.35561128,\n",
       "       -0.21709315,  2.20776609,  0.8889774 ,  0.09308352,  0.48711883,\n",
       "       -1.5722218 ,  1.79951197, -1.65310123, -1.68726251, -1.6717612 ,\n",
       "        1.81350111, -1.07284413,  1.3298569 ,  1.68364053,  1.03405283,\n",
       "        0.94847316, -0.23508562, -1.28451368, -0.07296996,  1.30025698])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_obs[nontrivial_effect_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lmfit_y_t.predict(tr)\n",
    "var_y_t = sum((y - y_hat)**2)/(y.shape[0]-1-tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5888.45788407])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by raw VAE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_u_t_diff_org(i):\n",
    "    print(i)\n",
    "    t1 = copy.deepcopy(tr)\n",
    "    t1[:,i] = 1\n",
    "    u_t1 = encoder_z_mean.predict([t1, log_sigma_input], batch_size = batch_size)\n",
    "    t0 = copy.deepcopy(tr)\n",
    "    t0[:,i] = 0\n",
    "    u_t0 = encoder_z_mean.predict([t0, log_sigma_input], batch_size = batch_size)\n",
    "    return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "u_t_diff_org_all = [cal_u_t_diff_org(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(u_t_diff_org_all).to_csv('u_t_diff_org_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by ISE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mu_u_t_ise(t):\n",
    "    nsim = 100\n",
    "    u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "    u_samples = np.random.normal(loc = u_t_mean[0,0], scale = u_t_sigma, size = nsim)\n",
    "    # f(t|u) #\n",
    "    p_t_u = generator.predict(u_samples.reshape(nsim,1)) ## sim in rach row\n",
    "    f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "    # f(u) #\n",
    "    f_u = norm(loc=0, scale=1).pdf(u_samples)\n",
    "    # q(u|t) #\n",
    "    q_u_t = norm(loc=u_t_mean[0,0], scale=u_t_sigma).pdf(u_samples)\n",
    "    # w = f(t|u)f(u)/q(u|t) #\n",
    "    w = f_t_u*f_u/q_u_t\n",
    "    mu_u_t_ise = (u_samples*w).mean() / w.mean()\n",
    "    return mu_u_t_ise ## averaged (single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_u_t_diff(i):\n",
    "    print(i)\n",
    "    t1 = copy.deepcopy(tr)\n",
    "    t1[:,i] = 1\n",
    "    u_t1 = np.apply_along_axis(cal_mu_u_t_ise, 1, t1)\n",
    "    t0 = copy.deepcopy(tr)\n",
    "    t0[:,i] = 0\n",
    "    u_t0 = np.apply_along_axis(cal_mu_u_t_ise, 1, t0)\n",
    "    return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_t_diff_ise_all = [cal_u_t_diff(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_t_diff_ise_t1 = cal_u_t_diff(0) \n",
    "u_t_diff_ise_t2 = cal_u_t_diff(1) \n",
    "u_t_diff_ise_t3 = cal_u_t_diff(2) \n",
    "u_t_diff_ise_t4 = cal_u_t_diff(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(u_t_diff_org_all)[:4])\n",
    "print(pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "                    u_t_diff_ise_t3, u_t_diff_ise_t4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(u_t_diff_all_ise).to_csv('u_t_diff_all_ise.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "#               u_t_diff_ise_t3, u_t_diff_ise_t4]).to_csv('u_t_diff_ise_1234.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
