{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## strong confoundedness: large B and gamma\n",
    "## small true effect: small tau\n",
    "\n",
    "k = 500\n",
    "s = 3\n",
    "np.random.seed(123)\n",
    "B = np.random.uniform(low=-2, high=2, size=(k,s))\n",
    "# B  = np.zeros(shape = (k,s))\n",
    "# B = np.array([2, 0.5, -0.4, 0.2]).reshape(k, s)\n",
    "sigma2_tstar = 100\n",
    "\n",
    "gamma = np.array([100]*s).reshape(s,1)\n",
    "tau = np.random.uniform(low=-0.1, high=0.1, size=(k,1))\n",
    "sigma2_y = 10\n",
    "\n",
    "# set some tau to large values #\n",
    "nontrivial_effect_index = np.random.randint(low = 0, high = k, size = int(k*0.1))\n",
    "nontrivial_effect_index = np.unique(nontrivial_effect_index)\n",
    "nontrivial_effect = np.random.uniform(low=-2, high=2, size=(nontrivial_effect_index.shape[0],1))\n",
    "tau[nontrivial_effect_index] = nontrivial_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 50000\n",
    "# u = np.random.multivariate_normal(mean = np.full(s, 0), cov=np.identity(s), size=n)\n",
    "# tr_star = np.dot(u, np.transpose(B)) + \\\n",
    "#           np.random.multivariate_normal(mean = np.full(k, 0), cov=sigma2_tstar*np.identity(k), size = n)\n",
    "# tr = np.where(tr_star > 0, 1, 0)\n",
    "# y = np.dot(tr, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n).reshape(n,1)\n",
    "# # y = np.dot(tr_star, tau) + np.dot(u, gamma) + np.random.normal(loc=0, scale=np.sqrt(sigma2_y), size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB = np.dot(B, np.transpose(B))\n",
    "BB_w, BB_v = np.linalg.eig(BB) ## eigenvalues, eigevector\n",
    "BB_w[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cov(u|t*):\n",
      "[[ 0.12583743  0.01137063 -0.00124313]\n",
      " [ 0.01137063  0.13200804 -0.00338941]\n",
      " [-0.00124313 -0.00338941  0.12658369]]\n"
     ]
    }
   ],
   "source": [
    "## some theoretical values #\n",
    "coef_mu_u_tstar = np.dot(np.transpose(B), np.linalg.inv(np.dot(B, np.transpose(B)) + sigma2_tstar*np.identity(k)))\n",
    "cov_u_tstar = np.identity(s) - np.dot(coef_mu_u_tstar, B)\n",
    "print('Cov(u|t*):')\n",
    "print(cov_u_tstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. Var(y|t) [Var(y|t*)]: 3989.0533932258177\n",
      "confounding fraction in residual of Y: 0.9974931395962305\n"
     ]
    }
   ],
   "source": [
    "var_y_t = np.dot(np.dot(np.transpose(gamma), cov_u_tstar), gamma) + sigma2_y\n",
    "print('Approx. Var(y|t) [Var(y|t*)]: ' + str(var_y_t[0,0]))\n",
    "print('confounding fraction in residual of Y: ' + str(1 - sigma2_y/var_y_t[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0079136 , -0.08092806,  0.08204695,  0.03360369,  0.00199419,\n",
       "       -0.01374685, -0.0897069 , -0.05714116,  0.07121584,  0.02444088,\n",
       "       -0.06122498, -0.06215936, -0.0299749 ,  0.02305931,  0.00848305,\n",
       "        0.00540698,  0.07378616,  0.07225029,  0.0690397 ,  0.04049034,\n",
       "       -0.02150996, -0.04613115,  0.09113853, -0.09094301,  0.02367661,\n",
       "        0.06922482, -0.04561742,  0.09574921,  0.07718479, -0.06013438,\n",
       "       -0.04593814,  0.07946058, -0.07814964, -0.06230457,  0.22737075,\n",
       "       -0.07059713,  0.01450903,  0.01510904,  0.01927088, -0.04985972,\n",
       "        0.06421617,  0.09408872, -0.0444228 ,  0.07309866, -0.04138959,\n",
       "       -0.03021458,  0.05708869, -1.72425897, -0.08763849, -0.02267348,\n",
       "       -0.07406336, -0.08408945,  0.0177456 , -0.06320505, -1.33897426,\n",
       "        0.04581771, -0.08140086,  0.09879365, -0.03824867,  0.00912585,\n",
       "        0.0583699 ,  0.92088713, -0.03176725, -0.05874625, -0.02428434,\n",
       "        0.04698733,  0.04288538, -0.06359616, -0.78220437,  0.03764135,\n",
       "        0.04762998,  0.02720739, -0.06103546,  0.07361581, -0.04231045,\n",
       "       -0.02601564,  0.01169956,  0.0718757 ,  0.02993006,  0.04033754,\n",
       "       -0.05672634, -0.02948764, -0.07262132,  0.04738068,  0.08975156,\n",
       "        0.04697365, -1.26953849,  0.03867307, -0.01335496, -1.80573921,\n",
       "       -1.71966913,  0.04482506,  0.02448759,  0.07461957, -0.03493178,\n",
       "        0.03336902, -0.00634391, -0.07521005,  0.00821394,  0.0263819 ,\n",
       "       -0.59146753, -0.07494569, -0.08494337,  0.03296769, -1.94806865,\n",
       "        0.08481881, -0.02236283,  0.08766317,  0.06396069, -0.01519167,\n",
       "       -0.01873599, -0.03330424, -0.08617668, -0.09805477, -0.05090027,\n",
       "       -0.04907653,  0.04593221, -0.06852586,  0.03848433,  0.04606706,\n",
       "       -0.00805657, -0.0764693 ,  0.04390192,  0.08272945, -0.02676411,\n",
       "        0.05174796,  0.04838374, -0.08919157, -0.08885847,  0.77538658,\n",
       "        1.95423119,  0.08411238, -0.07166738,  0.01667804, -0.04358938,\n",
       "       -0.05545983,  0.04767771, -0.01736162, -0.01246158,  0.06888025,\n",
       "       -0.05226305,  0.00566848,  0.06554208, -0.07362224,  0.01923659,\n",
       "        0.06743802,  0.05289311,  0.07689642, -0.09348716, -1.65303916,\n",
       "       -0.0592127 ,  0.01140375,  0.05495578, -0.01596171,  0.04084074,\n",
       "        0.06808214,  0.08914374,  0.00901862, -0.03678001, -0.02199477,\n",
       "        0.70915755, -0.05193667,  0.07665288, -0.08843275,  0.0942651 ,\n",
       "       -0.07617856,  0.07667781,  0.00815434,  0.01226555,  0.09613119,\n",
       "        0.09027242,  0.05972163,  0.03031669,  0.03554028, -0.07330629,\n",
       "       -0.00651893, -0.01416684, -0.07961818, -0.09773431, -0.09674634,\n",
       "        0.04749332, -0.0105088 , -0.00188508, -0.04928868,  0.00816767,\n",
       "       -0.07455859, -0.06729005,  0.06352881, -0.09451413,  0.0832027 ,\n",
       "       -0.03236242,  0.08643168, -0.08726024, -0.77705981,  0.00406989,\n",
       "        0.08756097, -0.08199929,  0.06183861,  0.02260478,  0.02800161,\n",
       "        0.09846182, -0.03578344, -0.02796241,  0.00717078,  0.02501677,\n",
       "       -0.03765078,  0.04126471, -0.08357508, -0.06103904, -0.09734963,\n",
       "        0.04992402,  0.08957962,  0.0537422 , -0.06542578, -0.04322843,\n",
       "        0.03423566, -0.03811068, -0.01863631, -0.04883921, -0.16177302,\n",
       "        0.07389526, -0.05206143,  0.00590745,  0.03300073,  1.64104514,\n",
       "       -0.04940545,  0.01345348,  0.0650399 , -0.0544077 ,  0.07976955,\n",
       "        0.14563839, -1.76995231,  0.06269499,  0.06984971, -0.04366796,\n",
       "       -0.07696973, -1.46567845,  0.07615003, -0.00111956, -1.0285786 ,\n",
       "       -0.98942204,  0.05605043,  0.06484264,  0.07850714,  0.02835396,\n",
       "        0.07842408,  0.07337062, -0.04567125,  0.01770958, -0.04756825,\n",
       "        0.02756383,  0.0794295 , -0.00337208,  0.06491203, -0.09657909,\n",
       "       -1.2312683 ,  0.08884063,  0.83302244,  0.0419291 ,  0.06384619,\n",
       "       -0.07514641, -0.01711081, -0.05868332,  0.04931178, -0.01881703,\n",
       "       -0.09997281, -0.04747327, -0.05933951, -0.08504635,  0.67791959,\n",
       "        0.45247182, -0.05621372, -0.0711157 , -0.03524437, -0.07445948,\n",
       "        0.00463566,  0.07846723,  0.05559157, -0.07036216, -0.04020389,\n",
       "       -0.05256627,  0.0855272 , -0.04414265, -0.06612364, -0.00897657,\n",
       "       -0.98926708, -0.06829755, -0.09509788, -0.081047  ,  0.03018955,\n",
       "       -0.06731033, -0.04823132,  0.03514375,  0.07427242,  0.09053434,\n",
       "       -0.02328344, -0.05472571,  0.04782645,  0.04248478, -1.2222459 ,\n",
       "       -0.08042239,  0.07562887, -0.06486795,  0.04946708,  0.09908506,\n",
       "       -0.05364431,  0.04843596,  0.0548315 , -0.04380813,  0.08424609,\n",
       "       -0.23054374, -0.05422401, -0.04108412, -0.08198931, -0.05094956,\n",
       "        0.05743268,  1.91208518,  0.00132314,  1.48641395,  0.01833742,\n",
       "        0.02217407, -0.05634783, -0.08506975,  0.04035694,  0.02668726,\n",
       "       -0.00354997,  0.07908452,  0.09545388,  0.04647883,  0.09245467,\n",
       "       -0.07057108, -0.06809086,  0.50995641, -0.02469233,  0.01189202,\n",
       "        0.08894644,  0.02079495,  0.07400833,  0.04145928,  0.07117538,\n",
       "       -0.00594058,  0.08987605, -0.07494777,  0.06153423, -0.05294155,\n",
       "        0.02376282,  0.05525244,  0.78777153, -1.47880466,  1.89085774,\n",
       "       -0.06763414, -1.5461094 , -0.04936391,  0.03759285, -0.06187572,\n",
       "       -0.0904438 ,  0.05695847,  0.06348925, -0.06751403,  0.06141991,\n",
       "       -0.03439159,  0.01422948,  0.0203764 ,  0.08272648,  0.0140461 ,\n",
       "        0.0737913 , -0.01569594,  0.03063446, -1.36082929,  0.04244286,\n",
       "        0.08555092, -0.01456473,  0.02945791,  0.00885959, -0.09709982,\n",
       "        0.08155659,  0.06185807, -0.01690305, -0.08180634, -0.02968093,\n",
       "        0.00512836, -0.05116533,  0.06650102, -0.04090459, -0.05901336,\n",
       "       -0.0654272 ,  0.04525392,  0.00932454,  0.03919488, -0.04193098,\n",
       "       -1.16390905,  0.07912145, -0.03391951,  0.08058983,  0.09325709,\n",
       "       -0.09609319, -0.00115391,  0.0244112 ,  1.43476655, -0.02996715,\n",
       "       -0.09439128,  0.0232983 , -0.00736834, -0.047417  ,  0.05551806,\n",
       "        0.00858288, -0.03351455,  0.08706058,  0.09516687, -0.06351979,\n",
       "        0.09567836,  0.0175208 ,  0.07507169, -0.04170049, -0.09067845,\n",
       "       -0.09748298, -0.01381378, -1.04635961, -0.01090371,  0.04879405,\n",
       "        0.03366685,  0.0463911 ,  0.01891618,  0.09079796, -0.05197947,\n",
       "        0.0807653 ,  0.09634574, -0.09929039, -0.01377244,  0.03521444,\n",
       "        0.02290711,  1.7091457 ,  0.03207362,  1.38186164,  0.08526586,\n",
       "        0.08042289,  0.06683719, -0.01852506,  0.03949964,  0.09119998,\n",
       "       -0.05202279, -0.00240375,  0.05151424,  0.87245279,  0.05674962,\n",
       "       -0.04719791, -0.07383491, -0.00575748,  0.03296292,  0.01950928,\n",
       "        0.0221351 , -0.08146763, -0.00173624, -0.05950699,  0.04213561,\n",
       "        0.01168383,  0.02015551,  0.01911338, -0.0387573 , -0.08332616,\n",
       "       -0.01738219, -0.04045172,  0.0139415 , -0.05537245, -0.05525964,\n",
       "       -0.0349688 , -0.0653351 , -0.01959785,  0.06480344,  0.07040088,\n",
       "        1.11250327, -0.03385397,  0.02264242, -0.0546103 ,  0.08389538,\n",
       "        0.04132667, -0.05594201,  0.00163242, -0.08524483, -0.06240518,\n",
       "       -0.02089042,  0.05015773, -0.07180021, -0.09270756,  0.07834011,\n",
       "       -0.03292489, -0.03392768,  0.00502148, -0.01603405, -0.04095397,\n",
       "        0.04311586,  0.05843372,  0.06525593,  0.05400606,  0.03808556,\n",
       "       -1.41772389,  0.04633008, -0.04505772,  0.0443635 , -0.08042742])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True Treatment Effect\n",
    "tau.reshape(k,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect bias range:\n",
      "-0.7511657117030033\n",
      "0.7201117743876483\n",
      "obs effect range:\n",
      "-2.1567691647786456\n",
      "1.860814044224537\n"
     ]
    }
   ],
   "source": [
    "# Approx. Obs. Effect #\n",
    "effect_bias = np.dot(np.dot(np.transpose(gamma), coef_mu_u_tstar), \\\n",
    "                     np.identity(k)).reshape(k,)\n",
    "print('effect bias range:')\n",
    "print(effect_bias.min())\n",
    "print(effect_bias.max())\n",
    "effect_obs = tau.reshape(k,) + effect_bias\n",
    "print('obs effect range:')\n",
    "print(effect_obs.min())\n",
    "print(effect_obs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Effect Bias for Nontrivial Ones:     ' + str(effect_bias[nontrivial_effect_index]))\n",
    "print('Observed Effect for Nontrivial Ones: ' + str(effect_obs[nontrivial_effect_index]))\n",
    "print('True Effect for Nontrivial Ones:     ' + str(tau[nontrivial_effect_index].reshape(50,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(tau).to_csv('tau.csv', index = False)\n",
    "# pd.DataFrame(u).to_csv('u.csv', index = False)\n",
    "# pd.DataFrame(tr).to_csv('tr.csv', index = False)\n",
    "# pd.DataFrame(y).to_csv('y.csv', index = False)\n",
    "# pd.DataFrame(nontrivial_effect_index).to_csv('nontrivial_effect_index.csv', index = False)\n",
    "n = 50000\n",
    "u = pd.read_csv('u.csv').to_numpy()\n",
    "tr = pd.read_csv('tr.csv').to_numpy()\n",
    "y = pd.read_csv('y.csv').to_numpy()\n",
    "tau = pd.read_csv('tau.csv').to_numpy()\n",
    "nontrivial_effect_index = pd.read_csv('nontrivial_effect_index.csv').to_numpy().reshape(50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('simulation/Sparse_Effects_Setting/LatentDim2')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "original_dim = k\n",
    "latent_dim = 2\n",
    "intermediate_dim = 10\n",
    "epochs = 300\n",
    "epsilon_std = 1\n",
    "z_log_sigma_prior = np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder #\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim)(x)\n",
    "h = LeakyReLU(alpha = 0.1)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "# z_log_sigma = Dense(latent_dim)(h)\n",
    "## log_z_scale\n",
    "\n",
    "\n",
    "z_log_sigma_input = Input(batch_shape = (batch_size, 1))\n",
    "z_log_sigma = Dense(units = 1,  activation = \"linear\",\n",
    "                    kernel_initializer=initializers.Ones(),\n",
    "                    use_bias = False)(z_log_sigma_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from latent space #\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "# z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder #\n",
    "decoder_h1 = Dense(intermediate_dim)\n",
    "decoder_h2 = LeakyReLU(alpha = 0.1)\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h1(z)\n",
    "h_decoded = decoder_h2(h_decoded)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model([x, z_log_sigma_input], x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder_z_mean = Model([x, z_log_sigma_input], z_mean)\n",
    "encoder_z_log_sigma = Model([x, z_log_sigma_input], z_log_sigma)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h1(decoder_input)\n",
    "_h_decoded = decoder_h2(_h_decoded)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_metric(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * binary_crossentropy(x, x_decoded_mean)\n",
    "    return xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.RMSprop(learning_rate=0.0005)\n",
    "# opt = optimizers.Adam(learning_rate=0.001)\n",
    "vae.compile(optimizer=opt, loss=vae_loss, metrics = [recon_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tr_train, tr_test = train_test_split(tr,train_size=0.8)\n",
    "log_sigma_input_train = np.full((tr_train.shape[0], 1), z_log_sigma_prior)\n",
    "log_sigma_input_test = np.full((tr_test.shape[0], 1), z_log_sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "40000/40000 [==============================] - 2s 46us/step - loss: 346.7878 - recon_metric: 346.6329 - val_loss: 346.7037 - val_recon_metric: 346.5921\n",
      "Epoch 2/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 346.6741 - recon_metric: 346.5644 - val_loss: 346.6524 - val_recon_metric: 346.5317\n",
      "Epoch 3/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 346.5737 - recon_metric: 346.3895 - val_loss: 346.4505 - val_recon_metric: 346.1890\n",
      "Epoch 4/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 346.2181 - recon_metric: 345.8258 - val_loss: 345.9665 - val_recon_metric: 345.4911\n",
      "Epoch 5/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 345.6601 - recon_metric: 345.0705 - val_loss: 345.3829 - val_recon_metric: 344.7257\n",
      "Epoch 6/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 345.0640 - recon_metric: 344.3201 - val_loss: 344.8610 - val_recon_metric: 344.0788\n",
      "Epoch 7/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 344.5476 - recon_metric: 343.6891 - val_loss: 344.3543 - val_recon_metric: 343.4889\n",
      "Epoch 8/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 344.1491 - recon_metric: 343.2313 - val_loss: 344.0629 - val_recon_metric: 343.1641\n",
      "Epoch 9/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.8655 - recon_metric: 342.9254 - val_loss: 343.7909 - val_recon_metric: 342.8915\n",
      "Epoch 10/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.6437 - recon_metric: 342.7083 - val_loss: 343.5987 - val_recon_metric: 342.6919\n",
      "Epoch 11/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 343.4763 - recon_metric: 342.5454 - val_loss: 343.5067 - val_recon_metric: 342.6145\n",
      "Epoch 12/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.3512 - recon_metric: 342.4292 - val_loss: 343.3911 - val_recon_metric: 342.5062\n",
      "Epoch 13/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.2346 - recon_metric: 342.3241 - val_loss: 343.2538 - val_recon_metric: 342.3622\n",
      "Epoch 14/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.1442 - recon_metric: 342.2435 - val_loss: 343.1764 - val_recon_metric: 342.3032\n",
      "Epoch 15/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 343.0556 - recon_metric: 342.1648 - val_loss: 343.1602 - val_recon_metric: 342.2886\n",
      "Epoch 16/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.9976 - recon_metric: 342.1175 - val_loss: 343.0377 - val_recon_metric: 342.1888\n",
      "Epoch 17/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.9371 - recon_metric: 342.0660 - val_loss: 343.0285 - val_recon_metric: 342.1888\n",
      "Epoch 18/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.8889 - recon_metric: 342.0258 - val_loss: 342.9411 - val_recon_metric: 342.1131\n",
      "Epoch 19/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.8448 - recon_metric: 341.9899 - val_loss: 342.8877 - val_recon_metric: 342.0418\n",
      "Epoch 20/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.8010 - recon_metric: 341.9537 - val_loss: 342.8423 - val_recon_metric: 342.0086\n",
      "Epoch 21/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.7690 - recon_metric: 341.9303 - val_loss: 342.8337 - val_recon_metric: 342.0070\n",
      "Epoch 22/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.7387 - recon_metric: 341.9063 - val_loss: 342.8231 - val_recon_metric: 341.9904\n",
      "Epoch 23/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.7115 - recon_metric: 341.8832 - val_loss: 342.8266 - val_recon_metric: 342.0002\n",
      "Epoch 24/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.6851 - recon_metric: 341.8644 - val_loss: 342.7642 - val_recon_metric: 341.9440\n",
      "Epoch 25/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.6673 - recon_metric: 341.8507 - val_loss: 342.7578 - val_recon_metric: 341.9440\n",
      "Epoch 26/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.6427 - recon_metric: 341.8321 - val_loss: 342.7007 - val_recon_metric: 341.8987\n",
      "Epoch 27/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.6224 - recon_metric: 341.8166 - val_loss: 342.6963 - val_recon_metric: 341.8989\n",
      "Epoch 28/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.6059 - recon_metric: 341.8050 - val_loss: 342.6866 - val_recon_metric: 341.8917\n",
      "Epoch 29/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.5864 - recon_metric: 341.7895 - val_loss: 342.6589 - val_recon_metric: 341.8676\n",
      "Epoch 30/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.5729 - recon_metric: 341.7793 - val_loss: 342.6436 - val_recon_metric: 341.8554\n",
      "Epoch 31/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.5583 - recon_metric: 341.7680 - val_loss: 342.6522 - val_recon_metric: 341.8651\n",
      "Epoch 32/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.5420 - recon_metric: 341.7545 - val_loss: 342.6221 - val_recon_metric: 341.8386\n",
      "Epoch 33/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.5287 - recon_metric: 341.7443 - val_loss: 342.6095 - val_recon_metric: 341.8291\n",
      "Epoch 34/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.5202 - recon_metric: 341.7369 - val_loss: 342.5870 - val_recon_metric: 341.8115\n",
      "Epoch 35/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.5069 - recon_metric: 341.7249 - val_loss: 342.5969 - val_recon_metric: 341.8171\n",
      "Epoch 36/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4887 - recon_metric: 341.7094 - val_loss: 342.6097 - val_recon_metric: 341.8328\n",
      "Epoch 37/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4810 - recon_metric: 341.7032 - val_loss: 342.5684 - val_recon_metric: 341.7943\n",
      "Epoch 38/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4730 - recon_metric: 341.6964 - val_loss: 342.5482 - val_recon_metric: 341.7754\n",
      "Epoch 39/300\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 342.4603 - recon_metric: 341.6840 - val_loss: 342.5451 - val_recon_metric: 341.7762\n",
      "Epoch 40/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4479 - recon_metric: 341.6733 - val_loss: 342.5363 - val_recon_metric: 341.7663\n",
      "Epoch 41/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.4385 - recon_metric: 341.6655 - val_loss: 342.5106 - val_recon_metric: 341.7440\n",
      "Epoch 42/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4283 - recon_metric: 341.6567 - val_loss: 342.5378 - val_recon_metric: 341.7677\n",
      "Epoch 43/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.4274 - recon_metric: 341.6537 - val_loss: 342.5080 - val_recon_metric: 341.7416\n",
      "Epoch 44/300\n",
      "40000/40000 [==============================] - 1s 26us/step - loss: 342.4093 - recon_metric: 341.6388 - val_loss: 342.5238 - val_recon_metric: 341.7542\n",
      "Epoch 45/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.4033 - recon_metric: 341.6328 - val_loss: 342.5331 - val_recon_metric: 341.7722\n",
      "Epoch 46/300\n",
      "40000/40000 [==============================] - 1s 27us/step - loss: 342.3948 - recon_metric: 341.6243 - val_loss: 342.5424 - val_recon_metric: 341.7774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdeaadb86d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting VAE #\n",
    "vae.fit([tr_train,log_sigma_input_train], tr_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([tr_test, log_sigma_input_test], tr_test),\n",
    "        callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigma_input = np.full((tr.shape[0], 1), z_log_sigma_prior)\n",
    "u_t_mean = encoder_z_mean.predict([tr, log_sigma_input], batch_size = batch_size)\n",
    "u_t_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "u_pcs = PCA(n_components = latent_dim).fit_transform(u)\n",
    "uhat_pcs = PCA(n_components = latent_dim).fit_transform(u_t_mean)\n",
    "for i in range(latent_dim):\n",
    "    print(i)\n",
    "    print(pearsonr(u_pcs[:,i], uhat_pcs[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the coef of imput_log_sigma #\n",
    "encoder_z_log_sigma.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output sigma_u_t #\n",
    "u_log_sigma = encoder_z_log_sigma.predict([tr_train, log_sigma_input_train], batch_size = batch_size)\n",
    "# print(u_log_sigma)\n",
    "u_t_sigma = np.exp(u_log_sigma)\n",
    "u_t_sigma[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output sigma_u_t #\n",
    "np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Sampling Estimates (ISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae ouput sigma_u_t #\n",
    "u_t_sigma = np.exp(z_log_sigma_prior*encoder_z_log_sigma.layers[1].get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_t_u_ise(i):\n",
    "    if (i % 1000 == 0): \n",
    "        print(i)\n",
    "    nsim = 100\n",
    "    t = tr[i]\n",
    "    u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "    u_samples = np.random.multivariate_normal(mean = u_t_mean[0], \\\n",
    "                                cov = u_t_sigma*np.identity(latent_dim), size = nsim)\n",
    "    # f(t|u) #\n",
    "    p_t_u = generator.predict(u_samples) ## sim in rach row\n",
    "    f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "    # f(u) #\n",
    "    f_u = multivariate_normal(mean=[0]*latent_dim, cov=np.identity(latent_dim)).pdf(u_samples)\n",
    "    # q(u|t) #\n",
    "    q_u_t = multivariate_normal(mean = u_t_mean[0], cov = u_t_sigma*np.identity(latent_dim)).pdf(u_samples)\n",
    "    # w = f(t|u)f(u)/q(u|t) #\n",
    "    nsamples = nsim\n",
    "    w = 10**(np.log10(f_t_u)+np.log10(f_u)-np.log10(q_u_t)).to_numpy().reshape(nsamples,1)\n",
    "    weight = 10**(np.log10(w) - np.log10(w.mean()))\n",
    "    mu_u_t_ise = (u_samples*weight).mean(axis=0)\n",
    "    mu_uu_t_ise = (np.apply_along_axis(lambda x: np.outer(x,x),1,u_samples)*weight.reshape(nsamples,1,1)).mean(axis=0)\n",
    "    cov_u_t_ise = mu_uu_t_ise - np.outer(mu_u_t_ise, mu_u_t_ise)\n",
    "    #     mu_u2_t_ise = (u_samples**2 * weight).mean(axis=0)\n",
    "    #     var_u_t_ise = mu_u2_t_ise - mu_u_t_ise**2\n",
    "    result = np.array([u_t_mean[0], mu_u_t_ise, cov_u_t_ise, weight.reshape(len(w))], dtype=object)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ise_results = [f_t_u_ise(i) for i in range(tr.shape[0])]\n",
    "# mu_u_t (VAE output); mu_u_t_ise, var_u_t_ise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_u_t_output = np.empty((0, latent_dim))\n",
    "mu_u_t_ise = np.empty((0, latent_dim))\n",
    "cov_u_t_ise = np.empty((0, latent_dim, latent_dim))\n",
    "for i in range(len(ise_results)):\n",
    "    mu_u_t_output = np.append(mu_u_t_output, ise_results[i][0].reshape(1,latent_dim), axis = 0)\n",
    "    mu_u_t_ise = np.append(mu_u_t_ise, ise_results[i][1].reshape(1,latent_dim), axis = 0)\n",
    "    cov_u_t_ise = np.append(cov_u_t_ise, ise_results[i][2].reshape(1,latent_dim,latent_dim), axis = 0)\n",
    "weight_mat = [ise_results[i][3] for i in range(len(ise_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07991785, 0.17920474])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output E(u|t) #\n",
    "mu_u_t_output.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02162568, -0.03767887])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE E(u|t) #\n",
    "mu_u_t_ise.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08653286989680717\n"
     ]
    }
   ],
   "source": [
    "# output Var(u|t) #\n",
    "print(u_t_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17475353, -0.00362843],\n",
       "       [-0.00362843,  0.1887586 ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISE Cov(u|t) #\n",
    "cov_u_t_ise_ave = cov_u_t_ise.mean(axis=0)\n",
    "cov_u_t_ise_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mu_u_t_ise).to_csv('mu_u_t_ise.csv', index = False)\n",
    "pd.DataFrame(cov_u_t_ise_ave).to_csv('cov_u_t_ise.csv', index = False)\n",
    "# mu_u_t_ise = pd.read_csv('mu_u_t_ise.csv').to_numpy()\n",
    "# cov_u_t_ise = pd.read_csv('cov_u_t_ise.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96881502, 0.01266577],\n",
       "       [0.01266577, 1.04592592]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "cov_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISE Cov(U|t):\n",
      "[[ 0.17475353 -0.00362843]\n",
      " [-0.00362843  0.1887586 ]]\n",
      "Cov(U):\n",
      "[[0.96881502 0.01266577]\n",
      " [0.01266577 1.04592592]]\n",
      "var(u|t)/var(u) for each dimension of U\n",
      "[0.18037863 0.18047034]\n"
     ]
    }
   ],
   "source": [
    "print('ISE Cov(U|t):')\n",
    "print(cov_u_t_ise_ave)\n",
    "# Cov(u) = E(Cov(u|t)) + Cov(E(u|t))\n",
    "cov_u = cov_u_t_ise.mean(axis=0) + np.cov(np.transpose(mu_u_t_ise))\n",
    "print('Cov(U):')\n",
    "print(cov_u)\n",
    "print('var(u|t)/var(u) for each dimension of U')\n",
    "print(np.diag(cov_u_t_ise_ave)/np.diag(cov_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting LM with Y ~ T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lmfit_y_t = LinearRegression().fit(tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficientï¼š tau_naive #\n",
    "tau_naive = lmfit_y_t.coef_.reshape(k,1)  # lmfit_y_t.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.85284773,  -0.91089822,  12.09315041,  -3.25899139,\n",
       "        -5.4797407 ,   0.04407081,  -7.36560428,  -6.34271326,\n",
       "         1.02171412, -13.13578741,  -0.81916312,   6.95351223,\n",
       "        -6.64761181,   9.55408621,  -5.28890334,  -8.35829301,\n",
       "       -11.70187037,  -4.04250111,  -0.03879425, -11.70187037,\n",
       "        -1.34684483,  -0.07948535,  -4.37678675,   5.5480885 ,\n",
       "        -7.23402651,  -6.62003685,   4.65643001,  10.25216207,\n",
       "        -4.34226198,  -3.64125522,   0.11976007,  10.25216207,\n",
       "       -13.14796222,  -9.29432922,  -6.37545183,  -2.92180524,\n",
       "         1.2008204 ,  -3.36721287,  -8.44000453, -13.45396435,\n",
       "        10.40681793,  -1.7186441 ,  -7.6641055 ,   8.42386768,\n",
       "         5.18251881,  -3.52986793,  -6.43146727,   1.00636515,\n",
       "        -3.6737733 ,   8.76517495])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_naive[nontrivial_effect_index].reshape(50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.99730088, -0.78821602, -0.2690769 , -1.70964132,  0.70083636,\n",
       "        1.55689782,  0.61881964, -2.12624376, -1.19229545, -1.60997656,\n",
       "        0.37224323, -1.56164907, -0.06631463, -1.45361638, -0.17410913,\n",
       "       -1.65725249, -2.15676916,  0.28632708,  0.80995587, -2.15676916,\n",
       "       -0.96666616,  0.80046988,  1.68725429,  0.29580761, -0.58177937,\n",
       "       -1.95190984, -1.3056579 ,  0.9736005 , -1.58162429,  1.30140116,\n",
       "        1.39531223,  0.9736005 , -1.95826584, -1.4632325 , -1.32923129,\n",
       "        1.86081404, -1.31359166, -0.17810752,  1.38271247, -0.28048141,\n",
       "       -0.61068748,  0.68267302, -0.97075633,  1.22266602,  1.80264518,\n",
       "       -1.581754  ,  1.67536714,  1.24571347, -1.49538273,  0.2223988 ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_obs[nontrivial_effect_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lmfit_y_t.predict(tr)\n",
    "var_y_t = sum((y - y_hat)**2)/(y.shape[0]-1-tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5888.45788407])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by raw VAE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_u_t_diff_org(i):\n",
    "    print(i)\n",
    "    t1 = copy.deepcopy(tr)\n",
    "    t1[:,i] = 1\n",
    "    u_t1 = encoder_z_mean.predict([t1, log_sigma_input], batch_size = batch_size)\n",
    "    t0 = copy.deepcopy(tr)\n",
    "    t0[:,i] = 0\n",
    "    u_t0 = encoder_z_mean.predict([t0, log_sigma_input], batch_size = batch_size)\n",
    "    return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "u_t_diff_org_all = [cal_u_t_diff_org(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(u_t_diff_org_all).to_csv('u_t_diff_org_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $E(U|Ti=1) - E(U|Ti=0)$ by ISE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cal_mu_u_t_ise(t):\n",
    "#     nsim = 100\n",
    "#     u_t_mean = encoder_z_mean.predict([t.reshape(1,k), np.array([[z_log_sigma_prior]])])\n",
    "#     u_samples = np.random.normal(loc = u_t_mean[0,0], scale = u_t_sigma, size = nsim)\n",
    "#     # f(t|u) #\n",
    "#     p_t_u = generator.predict(u_samples.reshape(nsim,1)) ## sim in rach row\n",
    "#     f_t_u = pd.DataFrame(p_t_u).apply(lambda x: np.prod(x**t * (1-x)**(1-t)), axis=1) ## sim in rach row\n",
    "#     # f(u) #\n",
    "#     f_u = norm(loc=0, scale=1).pdf(u_samples)\n",
    "#     # q(u|t) #\n",
    "#     q_u_t = norm(loc=u_t_mean[0,0], scale=u_t_sigma).pdf(u_samples)\n",
    "#     # w = f(t|u)f(u)/q(u|t) #\n",
    "#     w = f_t_u*f_u/q_u_t\n",
    "#     mu_u_t_ise = (u_samples*w).mean() / w.mean()\n",
    "#     return mu_u_t_ise ## averaged (single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# def cal_u_t_diff(i):\n",
    "#     print(i)\n",
    "#     t1 = copy.deepcopy(tr)\n",
    "#     t1[:,i] = 1\n",
    "#     u_t1 = np.apply_along_axis(cal_mu_u_t_ise, 1, t1)\n",
    "#     t0 = copy.deepcopy(tr)\n",
    "#     t0[:,i] = 0\n",
    "#     u_t0 = np.apply_along_axis(cal_mu_u_t_ise, 1, t0)\n",
    "#     return (u_t1 - u_t0).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_t_diff_ise_all = [cal_u_t_diff(i) for i in range(tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_t_diff_ise_t1 = cal_u_t_diff(0) \n",
    "# u_t_diff_ise_t2 = cal_u_t_diff(1) \n",
    "# u_t_diff_ise_t3 = cal_u_t_diff(2) \n",
    "# u_t_diff_ise_t4 = cal_u_t_diff(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(u_t_diff_org_all)[:4])\n",
    "# print(pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "#                     u_t_diff_ise_t3, u_t_diff_ise_t4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(u_t_diff_all_ise).to_csv('u_t_diff_all_ise.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([u_t_diff_ise_t1, u_t_diff_ise_t2, \\\n",
    "#               u_t_diff_ise_t3, u_t_diff_ise_t4]).to_csv('u_t_diff_ise_1234.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
